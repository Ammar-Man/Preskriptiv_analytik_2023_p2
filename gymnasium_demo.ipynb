{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7dd0ff8",
   "metadata": {},
   "source": [
    "# [Farama Gymnasium](https://gymnasium.farama.org/)\n",
    "##### Previously OpenAI Gym\n",
    "\n",
    "Fully supported on Linux and MacOS, see [Github](https://github.com/Farama-Foundation/Gymnasium) for details.\n",
    "\n",
    "Most packages work on Windows. *Tested with clean Anaconda env & Python 3.11.5*\n",
    "\n",
    "MujoCo and Atari versions require seperate files, read [the documentation](https://gymnasium.farama.org/).\n",
    "\n",
    "Trying to install **Box2D** on Windows? [This](https://www.youtube.com/watch?v=gMgj4pSHLww) might help.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fce68c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (4.4.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium) (0.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b5a3371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium[classic-control] in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[classic-control]) (1.23.5)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[classic-control]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[classic-control]) (4.4.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from gymnasium[classic-control]) (0.0.4)\n",
      "Collecting pygame>=2.1.3 (from gymnasium[classic-control])\n",
      "  Downloading pygame-2.5.2-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Downloading pygame-2.5.2-cp310-cp310-win_amd64.whl (10.8 MB)\n",
      "   ---------------------------------------- 10.8/10.8 MB 22.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pygame\n",
      "Successfully installed pygame-2.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium[classic-control]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69064b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f970835",
   "metadata": {},
   "source": [
    "## CartPole\n",
    "This environment corresponds to the version of the cart-pole problem described by **Barto, Sutton, and Anderson** in “Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problem”. A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum is placed upright on the cart and the goal is to balance the pole by applying forces in the left and right direction on the cart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c81a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(300):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bf9b0b",
   "metadata": {},
   "source": [
    "## Mountain Car\n",
    "The Mountain Car MDP is a deterministic MDP that consists of a car placed stochastically at the bottom of a sinusoidal valley, with the only possible actions being the accelerations that can be applied to the car in either direction. The goal of the MDP is to strategically accelerate the car to reach the goal state on top of the right hill. There are two versions of the mountain car domain in gymnasium: one with discrete actions and one with continuous. This version is the one with discrete actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da3cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"MountainCar-v0\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(300):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a3f364",
   "metadata": {},
   "source": [
    "## Blackjack\n",
    "\n",
    "The game starts with the dealer having one face up and one face down card, while the player has two face up cards. All cards are drawn from an infinite deck (i.e. with replacement).\n",
    "\n",
    "The card values are:\n",
    "- Face cards (Jack, Queen, King) have a point value of 10.\n",
    "- Aces can either count as 11 (called a ‘usable ace’) or 1.\n",
    "- Numerical cards (2-9) have a value equal to their number.\n",
    "\n",
    "The player has the sum of cards held. The player can request additional cards (hit) until they decide to stop (stick) or exceed 21 (bust, immediate loss).\n",
    "\n",
    "After the player sticks, the dealer reveals their facedown card, and draws cards until their sum is 17 or greater. If the dealer goes bust, the player wins.\n",
    "\n",
    "If neither the player nor the dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21.\n",
    "\n",
    "This environment corresponds to the version of the **blackjack problem described in Example 5.1 in Reinforcement Learning: An Introduction by Sutton and Barto**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96672853",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(10):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d68c4bd",
   "metadata": {},
   "source": [
    "## Frozen Lake\n",
    "The game starts with the player at location [0,0] of the frozen lake grid world with the goal located at far extent of the world e.g. [3,3] for the 4x4 environment.\n",
    "\n",
    "Holes in the ice are distributed in set locations when using a pre-determined map or in random locations when a random map is generated.\n",
    "\n",
    "The player makes moves until they reach the goal or fall in a hole.\n",
    "\n",
    "The lake is slippery (unless disabled) so the player may move perpendicular to the intended direction sometimes (see is_slippery).\n",
    "\n",
    "Randomly generated worlds will always have a path to the goal.\n",
    "\n",
    "Elf and stool from https://franuka.itch.io/rpg-snow-tileset. All other assets by Mel Tillery http://www.cyaneus.com/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351d60d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(100):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2c76f",
   "metadata": {},
   "source": [
    "## Lunar Lander\n",
    "This environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n",
    "\n",
    "There are two environment versions: discrete or continuous. The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eb111",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(300):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dcba18",
   "metadata": {},
   "source": [
    "## Bipedal Walker\n",
    "This is a simple 4-joint walker robot environment. There are two versions:\n",
    "\n",
    "- Normal, with slightly uneven terrain.\n",
    "- Hardcore, with ladders, stumps, pitfalls.\n",
    "\n",
    "To solve the normal version, you need to get 300 points in 1600 time steps. To solve the hardcore version, you need 300 points in 2000 time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef9270",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(200):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fde29",
   "metadata": {},
   "source": [
    "## Car Racing\n",
    "The easiest control task to learn from pixels - a top-down racing environment. The generated track is random every episode.\n",
    "\n",
    "Some indicators are shown at the bottom of the window along with the state RGB buffer. From left to right: true speed, four ABS sensors, steering wheel position, and gyroscope. To play yourself (it’s rather fast for humans), type:\n",
    "```\n",
    "python gymnasium/envs/box2d/car_racing.py\n",
    "```\n",
    "\n",
    "\n",
    "Remember: it’s a powerful rear-wheel drive car - don’t press the accelerator and turn at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f7d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CarRacing-v2\", render_mode=\"human\")\n",
    "observation, info = env.reset(seed=42)\n",
    "for _ in range(200):\n",
    "   action = env.action_space.sample()  # this is where you would insert your policy\n",
    "   observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "   if terminated or truncated:\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a258f5",
   "metadata": {},
   "source": [
    "## And many many more...\n",
    "Check out the [full documentation](https://gymnasium.farama.org/) for many more environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d659c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
