{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc3471e",
   "metadata": {},
   "source": [
    "## GridWorld\n",
    "Ph.D Leonarod A, Espinosa, M.Sc Andrej Scherbakov-Parland, BIT Kristoffer Kuvaja Adolfsson\n",
    "\n",
    "### Bibliography:\n",
    "\n",
    "* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n",
    "http://incompleteideas.net/book/bookdraft2017nov5.pdf  (chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d331c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    \"\"\" Gridworld defined by m x n matrix with\n",
    "    terminal states at top left corner and bottom right corner.\n",
    "    State transitions are deterministic; attempting to move\n",
    "    off the grid leaves the state unchanged, and rewards are -1 on\n",
    "    each step. \n",
    "\n",
    "    In this implementation we model the environment like a game\n",
    "    where an agent moves around.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.grid = np.zeros((m,n))\n",
    "        self.stateSpace = [i+1 for i in range(self.m*self.n-2)]\n",
    "        self.stateSpacePlus = [i for i in range(self.m*self.n)]        \n",
    "        self.actionSpace = {'up': -self.m, 'down': self.m, \n",
    "                            'left': -1, 'right': 1}\n",
    "        self.p = self.initP() # probability functions\n",
    "        self.agentPosition = np.random.choice(self.stateSpace)\n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 1\n",
    "\n",
    "    def getAgentRowAndColumn(self):\n",
    "        x = self.agentPosition // self.m\n",
    "        y = self.agentPosition % self.n\n",
    "        return x, y\n",
    "\n",
    "    def setState(self, state):\n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 0            \n",
    "        self.agentPosition = state        \n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 1  \n",
    "\n",
    "    def offGridMove(self, newState, oldState):\n",
    "        # if we move into a row not in the grid\n",
    "        if newState not in self.stateSpacePlus:\n",
    "            return True\n",
    "        # if we're trying to wrap around to next row\n",
    "        elif oldState % self.m == 0 and newState  % self.m == self.m - 1:\n",
    "            return True\n",
    "        elif oldState % self.m == self.m - 1 and newState % self.m == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False        \n",
    "    def step(self, action):        \n",
    "        resultingState = self.agentPosition + self.actionSpace[action]\n",
    "        if not self.offGridMove(resultingState, self.agentPosition):\n",
    "            self.setState(resultingState)\n",
    "            return resultingState, -1, self.isTerminalState(resultingState), None\n",
    "        else:\n",
    "            return self.agentPosition, -1, self.isTerminalState(self.agentPosition), None\n",
    "    \n",
    "    def isTerminalState(self, state):\n",
    "        return state in self.stateSpacePlus and state not in self.stateSpace      \n",
    "\n",
    "    def initP(self):\n",
    "        \"\"\" construct state transition probabilities for\n",
    "        use in value function. P(s', r|s, a) is a dictionary\n",
    "        with keys corresponding to the functional arguments.\n",
    "        values are either 1 or 0.\n",
    "        Translations that take agent off grid leave the state unchanged.\n",
    "        (s', r|s, a)\n",
    "        (1, -1|1, 'up') = 1\n",
    "        (1, -1|2, 'left') = 1\n",
    "        (1, -1|3, 'left') = 0        \n",
    "        \"\"\"\n",
    "        P = {}\n",
    "        for state in self.stateSpace:\n",
    "            for action in self.actionSpace:\n",
    "                resultingState = state + self.actionSpace[action]\n",
    "                key = (state, -1, state, action) if self.offGridMove(resultingState, state) \\\n",
    "                                                 else (resultingState, -1, state, action)\n",
    "                P[key] = 1\n",
    "        return P\n",
    "\n",
    "    def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.grid:\n",
    "            for col in row:\n",
    "                if col == 0:\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 1:\n",
    "                    print('X', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bbc5f",
   "metadata": {},
   "source": [
    "## Del 1 - Utvärdera policyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca77fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(grid, V, policy, GAMMA, THETA):\n",
    "    # policy evaluation for the random choice in gridworld\n",
    "    converged = False\n",
    "    iterations = 0 # <---\n",
    "    while not converged:\n",
    "        DELTA = 0\n",
    "        for state in grid.stateSpace:\n",
    "            oldV = V[state]\n",
    "            total = 0\n",
    "            weight = 1 / len(policy[state])           \n",
    "            for action in policy[state]:\n",
    "                grid.setState(state)\n",
    "                newState, reward, _, _ = grid.step(action)\n",
    "                key = (newState, reward, state, action)\n",
    "                total += weight*grid.p[key]*(reward+GAMMA*V[newState])\n",
    "            V[state] = total\n",
    "            DELTA = max(DELTA, np.abs(oldV-V[state]))\n",
    "            converged = True if DELTA < THETA else False\n",
    "        iterations = iterations + 1 # <---\n",
    "    print('iterations: ', iterations) # <----\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printV(V, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy \n",
    "            print('%.2f' % V[state], end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6057c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(m, n, x, y):\n",
    "    grid = GridWorld(m,n) # Rutnät m x n\n",
    "    THETA = x # Konvergensvillkor\n",
    "    GAMMA = y\n",
    "    \n",
    "    # initialize V(s)\n",
    "    V = {}\n",
    "    for state in grid.stateSpacePlus:        \n",
    "        V[state] = 0\n",
    "    \n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = [key for key in grid.actionSpace.keys()]\n",
    "\n",
    "    \n",
    "    V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n",
    "    printV(V, grid)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8dac63",
   "metadata": {},
   "source": [
    "### Hands-on del 1\n",
    "- Implementera utvärderingspolicyfunktionen (evaluatePolicy) i en python-notebook.\n",
    "- Testa koden för 4x4, 8x8 och 10x10 rutnät.\n",
    "    - Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "    - Vad händer med tiden?\n",
    "- Vad händer om gamma är lika med 0,5, 0,9 för 4x4 och 8x8? Summera och  gör slutsatser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b4f4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2d1be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab057bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9821e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fb473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00454e56",
   "metadata": {},
   "source": [
    "## Del 2 - Förbättra policyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvePolicy(grid, V, policy, GAMMA):\n",
    "    stable = True\n",
    "    newPolicy = {}\n",
    "    for state in grid.stateSpace:       \n",
    "        oldActions = policy[state]                \n",
    "        value = []\n",
    "        newAction = []\n",
    "        for action in policy[state]:\n",
    "            grid.setState(state)\n",
    "            weight = 1 / len(policy[state])\n",
    "            newState, reward, _, _ = grid.step(action)\n",
    "            key = (newState, reward, state, action)\n",
    "            value.append(np.round(weight*grid.p[key]*(reward+GAMMA*V[newState]), 2))\n",
    "            newAction.append(action)\n",
    "        value = np.array(value)        \n",
    "        best = np.where(value == value.max())[0]        \n",
    "        bestActions = [newAction[item] for item in best] \n",
    "        newPolicy[state] = bestActions\n",
    "\n",
    "        if oldActions != bestActions:\n",
    "            stable = False\n",
    "        \n",
    "    return stable, newPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adc0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_iteration(m, n, x, y):\n",
    "\n",
    "    grid = GridWorld(m,n)\n",
    "    THETA = x\n",
    "    GAMMA = y\n",
    "    \n",
    "    # initialize V(s)\n",
    "\n",
    "    V = {}\n",
    "    for state in grid.stateSpacePlus:        \n",
    "        V[state] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = [key for key in grid.actionSpace.keys()]\n",
    "\n",
    "    # main loop for policy improvement\n",
    "    stable = False\n",
    "    while not stable:\n",
    "        V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n",
    "        stable, policy = improvePolicy(grid, V, policy, GAMMA)       \n",
    "        V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n",
    "\n",
    "        printV(V, grid) \n",
    "        time.sleep(2)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    printV(V,grid)\n",
    "\n",
    "    for state in policy:\n",
    "        print(state,policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6408bc",
   "metadata": {},
   "source": [
    "### Hands-on del 2.\n",
    "\n",
    "- Implementera den förbättrade policyfunktionen (improvePolicy).\n",
    "- Testa koden för 4x4, 8x8 och 10x10 rutnät.\n",
    "    - Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "    - Vad händer med tiden?\n",
    "- Hur olika är policyerna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baff7ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f83f88f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7534fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9d82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3421506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ac0a82f",
   "metadata": {},
   "source": [
    "## Del 3 - Värde iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd37880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateValues(grid, V, policy, GAMMA, THETA):\n",
    "    converged = False\n",
    "    iterations = 0\n",
    "    while not converged:\n",
    "        DELTA = 0\n",
    "        for state in grid.stateSpace:\n",
    "            oldV = V[state]\n",
    "            newV = []            \n",
    "            for action in grid.actionSpace:\n",
    "                grid.setState(state)\n",
    "                newState, reward, _, _ = grid.step(action)\n",
    "                key = (newState, reward, state, action) \n",
    "                newV.append(grid.p[key]*(reward+GAMMA*V[newState]))                \n",
    "            newV = np.array(newV)\n",
    "            bestV = np.where(newV == newV.max())[0]\n",
    "            bestState = np.random.choice(bestV)\n",
    "            V[state] = newV[bestState]\n",
    "            DELTA = max(DELTA, np.abs(oldV-V[state]))\n",
    "            converged = True if DELTA < THETA else False\n",
    "        iterations = iterations + 1\n",
    "    print('iterations: ', iterations)\n",
    "\n",
    "    for state in grid.stateSpace:\n",
    "        newValues = []\n",
    "        actions = []\n",
    "        for action in grid.actionSpace:\n",
    "            grid.setState(state)\n",
    "            newState, reward, _, _ = grid.step(action)\n",
    "            key = (newState, reward, state, action)\n",
    "            newValues.append(grid.p[key]*(reward+GAMMA*V[newState]))\n",
    "            actions.append(action)\n",
    "        newValues = np.array(newValues)\n",
    "        bestActionIDX = np.where(newValues == newValues.max())[0]\n",
    "        bestActions = actions[np.random.choice(bestActionIDX)]\n",
    "        bestActions = actions[bestActionIDX[0]]\n",
    "        policy[state] = bestActions\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd10bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_iteration(m, n, x, y):\n",
    "\n",
    "    grid = GridWorld(m,n)\n",
    "    THETA = x\n",
    "    GAMMA = y\n",
    "\n",
    "    # initialize V(s)\n",
    "    V = {}\n",
    "    for state in grid.stateSpacePlus:        \n",
    "        V[state] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = [key for key in grid.actionSpace.keys()]\n",
    "\n",
    "    for i in range(2):\n",
    "        V, policy = iterateValues(grid, V, policy, GAMMA, THETA)\n",
    "        printV(V, grid)   \n",
    "        time.sleep(2)\n",
    "        clear_output(wait=True)\n",
    "    # Final   \n",
    "\n",
    "    printV(V, grid) \n",
    "    print()\n",
    "    for state in policy:\n",
    "        print(state, policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d888545",
   "metadata": {},
   "source": [
    "### Hands-on del 3.\n",
    "\n",
    "- Implementera ännu en förbättrad policyfunktion (iterateValues).\n",
    "- Testa koden för 4x4, 8x8 och 10x10 rutnät.\n",
    "    - Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "    - Vad händer med tiden?\n",
    "- Hur olika är policyerna för gamma=0.5,0.9 i ett 8x8-rutnät?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6faea5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa28900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84845efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fd16b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
