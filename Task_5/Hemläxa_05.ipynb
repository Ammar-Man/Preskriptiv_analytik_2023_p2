{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2851194",
   "metadata": {},
   "source": [
    "## GridWorld\n",
    "Ph.D Leonarod A, Espinosa, M.Sc Andrej Scherbakov-Parland, BIT Kristoffer Kuvaja Adolfsson\n",
    "\n",
    "### Bibliography:\n",
    "\n",
    "* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n",
    "http://incompleteideas.net/book/bookdraft2017nov5.pdf  (chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1f756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07eafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilits\n",
    "def printV(V, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy \n",
    "            print('%.2f' % V[state], end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')\n",
    "\n",
    "def printPolicy(policy, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy \n",
    "            if state in grid.stateSpace:\n",
    "                string = ''.join(policy[state])\n",
    "                print(string, end='\\t')\n",
    "            else:\n",
    "                print('', end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')    \n",
    "\n",
    "def printQ(Q, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy            \n",
    "            if state != grid.m * grid.n - 1:\n",
    "                vals = [np.round(Q[state,action], 5) for action in grid.possibleActions]\n",
    "                print(vals, end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')\n",
    "\n",
    "def sampleReducedActionSpace(grid, action):\n",
    "    actions = grid.possibleActions[:]\n",
    "    actions.remove(action)\n",
    "    sample = np.random.choice(actions)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffbb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGrid(object):\n",
    "    def __init__(self, m, n, wind):         \n",
    "        self.grid = np.zeros((m,n))                            # representation of the grid\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.stateSpace = [i for i in range(self.m*self.n)]        \n",
    "        self.stateSpace.remove(28)                              # Terminal state\n",
    "        self.stateSpacePlus = [i for i in range(self.m*self.n)] # State space + terminal state\n",
    "        self.actionSpace = {'U': -self.m, 'D': self.m, \n",
    "                            'L': -1, 'R': 1}\n",
    "        self.possibleActions = ['U', 'D', 'L', 'R']\n",
    "        self.agentPosition = 0\n",
    "        self.wind = wind\n",
    "\n",
    "    def isTerminalState(self, state):\n",
    "        return state in self.stateSpacePlus and state not in self.stateSpace \n",
    "\n",
    "    def getAgentRowAndColumn(self):                               # position of agent\n",
    "        x = self.agentPosition // self.m\n",
    "        y = self.agentPosition % self.n\n",
    "        return x, y\n",
    "    \n",
    "    def setState(self, state):\n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 0            \n",
    "        self.agentPosition = state        \n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 1   \n",
    "    \n",
    "    def offGridMove(self, newState, oldState):\n",
    "        # if we move into a row not in the grid\n",
    "        if newState not in self.stateSpacePlus:\n",
    "            return True\n",
    "        # if we're trying to wrap around to next row\n",
    "        elif oldState % self.m == 0 and newState  % self.m == self.m - 1:\n",
    "            return True\n",
    "        elif oldState % self.m == self.m - 1 and newState % self.m == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False   \n",
    "        \n",
    "    # Include wind stenght.\n",
    "    def step22(self, action):\n",
    "        agentX, agentY = self.getAgentRowAndColumn()\n",
    "        if agentY >= 0 and agentY < len(self.wind):  # Kontrollera om agentY är ett giltigt index\n",
    "            resultingState = self.agentPosition + self.actionSpace[action] + self.wind[agentY] * self.actionSpace['U']\n",
    "            if resultingState < 0:  # If the wind is trying to push the agent off the grid\n",
    "                resultingState += self.m\n",
    "\n",
    "        if agentX > 0:\n",
    "            resultingState = self.agentPosition + self.actionSpace[action] + \\\n",
    "                            self.wind[agentY] * self.actionSpace['U']\n",
    "            if resultingState < 0: #if the wind is trying to push agent off grid\n",
    "                resultingState += self.m\n",
    "        else:\n",
    "            if action == 'L' or action == 'R':\n",
    "                resultingState = self.agentPosition + self.actionSpace[action]\n",
    "            else:\n",
    "                resultingState = self.agentPosition + self.actionSpace[action] + \\\n",
    "                            self.wind[agentY] * self.actionSpace['U']\n",
    "        #reward = -1 if not self.isTerminalState(resultingState) else 0\n",
    "        reward = -1\n",
    "        if not self.offGridMove(resultingState, self.agentPosition):\n",
    "            self.setState(resultingState)\n",
    "            return resultingState, reward, self.isTerminalState(resultingState), None\n",
    "        else:\n",
    "            return self.agentPosition, reward, self.isTerminalState(self.agentPosition), None\n",
    "        \n",
    "    def step(self, action):\n",
    "        agentX, agentY = self.getAgentRowAndColumn()\n",
    "\n",
    "        if agentY >= 0 and agentY < len(self.wind):  # Kontrollera om agentY är ett giltigt index\n",
    "            wind_effect = self.wind[agentY] * self.actionSpace['U']\n",
    "        else:\n",
    "            wind_effect = 0\n",
    "\n",
    "        if agentX > 0:\n",
    "            resultingState = self.agentPosition + self.actionSpace[action] + wind_effect\n",
    "        else:\n",
    "            if action == 'L' or action == 'R':\n",
    "                resultingState = self.agentPosition + self.actionSpace[action]\n",
    "            else:\n",
    "                resultingState = self.agentPosition + self.actionSpace[action] + wind_effect\n",
    "\n",
    "        if resultingState < 0:  # If the wind or action is trying to push the agent off the grid\n",
    "            resultingState += self.m\n",
    "\n",
    "        reward = -1\n",
    "\n",
    "        if not self.offGridMove(resultingState, self.agentPosition):\n",
    "            self.setState(resultingState)\n",
    "            return resultingState, reward, self.isTerminalState(resultingState), None\n",
    "        else:\n",
    "            return self.agentPosition, reward, self.isTerminalState(self.agentPosition), None\n",
    "\n",
    "    def reset(self):\n",
    "        self.agentPosition = 0\n",
    "        self.grid = np.zeros((self.m,self.n))\n",
    "        return self.agentPosition, False\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.grid:\n",
    "            for col in row:\n",
    "                if col == 0:\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 1:\n",
    "                    print('X', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216e193",
   "metadata": {},
   "source": [
    "## First visit Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2387e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_first_visit(X =6 ,loop=500,Y=1,  wind=[0, 0, 1, 2, 1, 0]):\n",
    "    print(\"GAMMA:\",Y)\n",
    "    print(\"Size:\",X,X)\n",
    "\n",
    "    grid = WindyGrid(X,X, wind)\n",
    "    GAMMA = 1.0\n",
    "    if(Y):\n",
    "        GAMMA = Y\n",
    "    \n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0                                                              \n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,  \n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited \n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done \n",
    "            # attempt to follow the policy. In this case choose an action \n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])    \n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state                   \n",
    "        for state, action, reward in reversed(memory): \n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list. \n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see \n",
    "                returns[state].append(G)         # if each state has been visited before. \n",
    "                V[state] = np.mean(returns[state]) \n",
    "                statesVisited.append(state)\n",
    "                \n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append \n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally, \n",
    "                #append that state to the list of statesVisited. \n",
    "    print(\"\\n\") \n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0c1eb",
   "metadata": {},
   "source": [
    "## Del 1:\n",
    "\n",
    "- Använd *first visit* Monte Carlo Metoden\n",
    "\n",
    "1. Öka vindstyrkan med en enhet.\n",
    "    - Hur ändras slutvärdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur ändras värdefunktion om man ändra gamma till:\n",
    "    - 𝛾=0.5\n",
    "    - 𝛾=0,9\n",
    "    - 𝛾=0,95\n",
    "\n",
    "\n",
    "3. Testa rutnätsvärlden i storlekarna:\n",
    "    - 8x8\n",
    "        - Ändra på vinden, vad händer med värdefunktion?\n",
    "        - Prova med 𝛾=0,9, vad händer med värdefunktion?\n",
    "    - 10x10\n",
    "        - Ändra på vinden, vad händer med värdefunktion?\n",
    "        - Prova med 𝛾=0,9, vad händer med värdefunktion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2885845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.5\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.99\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.95\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t0.00\t-1.74\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t0.00\t-1.96\t-1.94\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.99\t-9.98\t-9.99\t-9.97\t-9.96\t-9.92\t\n",
      "\n",
      "-10.00\t-9.99\t-9.98\t-9.98\t-9.94\t-9.78\t\n",
      "\n",
      "-9.99\t-10.00\t-9.98\t-9.99\t-9.91\t-9.43\t\n",
      "\n",
      "-9.99\t-10.00\t-9.99\t-9.97\t-9.89\t-8.43\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-9.95\t0.00\t-6.02\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t0.00\t-8.59\t-8.03\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.95\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-19.95\t-19.94\t-19.90\t-19.86\t-19.74\t-19.62\t\n",
      "\n",
      "-19.94\t-19.92\t-19.91\t-19.83\t-19.74\t-19.42\t\n",
      "\n",
      "-19.94\t-19.90\t-19.86\t-19.87\t-19.71\t-18.78\t\n",
      "\n",
      "-19.94\t-19.91\t-19.90\t-19.74\t-19.53\t-16.64\t\n",
      "\n",
      "-19.94\t-19.92\t-19.84\t-19.83\t0.00\t-11.16\t\n",
      "\n",
      "-19.91\t-19.87\t-19.90\t0.00\t-16.77\t-14.64\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit(6,500,0.5)\n",
    "MC_first_visit(6,500,0.9)\n",
    "MC_first_visit(6,500,0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Här är observationer:\n",
    "\n",
    "- 𝛾 = 0.5:\n",
    "Värdefunktionen verkar inte ge mycket vikt åt framtida belöningar. Det är tydligt eftersom värdena i den nedre högra delen av rutnätet är fortfarande ganska låga, även när det finns en positiv belöning där.\n",
    "\n",
    "- 𝛾 = 0.9:\n",
    "Här ser det ut som att systemet ger mer vikt åt framtida belöningar. Värdena i den nedre högra delen av rutnätet är lägre än i det första fallet, vilket tyder på att systemet tar hänsyn till de långsiktiga konsekvenserna.\n",
    "\n",
    "- 𝛾 = 0.95:\n",
    "Detta scenario verkar ge ännu mer vikt åt framtida belöningar. Värdena i den nedre högra delen av rutnätet är ännu lägre, och systemet verkar vara mer inriktat på att maximera de långsiktiga belöningarna.\n",
    "\n",
    "Sammanfattningsvis kan du säga att med ökande värden på gamma ger systemet mer vikt åt framtida belöningar och blir mer inriktat på att maximera de långsiktiga belöningarna jämfört med omedelbara belöningar. Detta är en typisk observation i förstärkningsinlärning, där valet av gamma påverkar agentens inlärningsbeteende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 8 8\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.98\t-9.97\t-9.96\t-9.91\t-9.86\t-9.77\t-9.81\t-9.75\t\n",
      "\n",
      "-9.98\t-9.98\t-9.96\t-9.93\t-9.88\t-9.60\t-9.60\t-9.64\t\n",
      "\n",
      "-9.97\t-9.97\t-9.97\t-9.95\t-9.86\t-9.10\t-9.32\t-9.49\t\n",
      "\n",
      "-9.96\t-9.95\t-9.95\t-9.96\t0.00\t-6.69\t-8.77\t-9.39\t\n",
      "\n",
      "-9.94\t-9.92\t-9.92\t-9.97\t-8.91\t-8.58\t-9.17\t-9.39\t\n",
      "\n",
      "-9.90\t-9.86\t-9.98\t-6.81\t-7.12\t-8.73\t-9.28\t-9.57\t\n",
      "\n",
      "-9.77\t-9.63\t-9.15\t-9.36\t-8.19\t-9.11\t-9.41\t-9.62\t\n",
      "\n",
      "-9.89\t-9.83\t-9.68\t0.00\t-8.91\t-9.35\t-9.53\t-9.68\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 8 8\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.99\t-9.98\t-9.98\t-9.96\t-9.92\t-9.84\t-9.78\t-9.81\t\n",
      "\n",
      "-9.99\t-9.98\t-9.98\t-9.96\t-9.87\t-9.66\t-9.63\t-9.66\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t-9.93\t-9.89\t-8.92\t-9.28\t-9.48\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t-9.96\t0.00\t-6.97\t-8.81\t-9.33\t\n",
      "\n",
      "-9.99\t-9.97\t-9.94\t-9.98\t-7.04\t-8.29\t-9.22\t-9.37\t\n",
      "\n",
      "-9.99\t-9.99\t-9.96\t-9.99\t-9.01\t-9.02\t-9.25\t-9.45\t\n",
      "\n",
      "-9.99\t-9.98\t-9.95\t0.00\t-7.85\t-8.91\t-9.35\t-9.55\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t0.00\t-8.64\t-9.18\t-9.51\t-9.71\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit(8,500,0.9)\n",
    "MC_first_visit(8,500,0.9,wind=[0,0,2,3,2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Första simuleringen:\n",
    "GAMMA: 0.9, Wind=[0,0,1,2,1,0]: Värdefunktionen påverkas av vind och gamma. Starkare vind leder till lägre värden, och systemet tar hänsyn till svårigheten att röra sig i den riktningen.\n",
    "\n",
    "Andra simuleringen:\n",
    "GAMMA: 0.9, Wind=[0,0,2,3,2,0]: Liknande påverkan som i den första simuleringen. Starkare vind ger lägre värden, och systemet tar hänsyn till ökad svårighet att navigera genom vinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb5c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 10 10\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.97\t-9.95\t-9.91\t-9.86\t-9.75\t-9.48\t-9.07\t-8.38\t-8.04\t-8.03\t\n",
      "\n",
      "-9.97\t-9.96\t-9.93\t-9.83\t-9.72\t-9.45\t-8.74\t-7.64\t-6.34\t-7.03\t\n",
      "\n",
      "-9.98\t-9.95\t-9.93\t-9.83\t-9.66\t-9.31\t-8.62\t-6.55\t0.00\t-5.47\t\n",
      "\n",
      "-9.97\t-9.96\t-9.92\t-9.84\t-9.57\t-9.39\t-9.26\t-7.76\t-6.49\t-7.34\t\n",
      "\n",
      "-9.96\t-9.95\t-9.93\t-9.82\t-9.53\t-9.59\t-9.38\t-8.84\t-8.31\t-8.88\t\n",
      "\n",
      "-9.95\t-9.91\t-9.89\t-9.86\t-9.89\t-9.70\t-9.54\t-9.55\t-9.16\t-9.43\t\n",
      "\n",
      "-9.98\t-9.95\t-9.92\t-9.53\t-9.68\t-9.79\t-9.79\t-9.69\t-9.47\t-9.52\t\n",
      "\n",
      "-9.97\t-9.96\t-9.92\t-10.00\t-9.67\t-9.82\t-9.79\t-9.81\t-9.74\t-9.68\t\n",
      "\n",
      "-9.95\t-9.96\t-9.89\t-9.99\t-9.90\t-9.90\t-9.96\t-9.87\t-9.77\t-9.89\t\n",
      "\n",
      "-9.97\t-9.99\t-10.00\t0.00\t-9.89\t-9.91\t-9.93\t-9.91\t-9.90\t-9.90\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 10 10\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.97\t-9.96\t-9.91\t-9.86\t-9.71\t-9.42\t-9.03\t-8.34\t-7.81\t-7.77\t\n",
      "\n",
      "-9.97\t-9.96\t-9.94\t-9.93\t-9.82\t-9.33\t-8.68\t-7.46\t-5.95\t-7.01\t\n",
      "\n",
      "-9.97\t-9.96\t-9.92\t-9.86\t-9.72\t-9.20\t-8.32\t-6.04\t0.00\t-5.09\t\n",
      "\n",
      "-9.98\t-9.96\t-9.93\t-9.94\t-9.64\t-9.41\t-8.67\t-7.63\t-6.06\t-6.29\t\n",
      "\n",
      "-9.98\t-9.96\t-9.97\t-9.87\t-9.49\t-9.49\t-9.12\t-8.72\t-8.32\t-7.79\t\n",
      "\n",
      "-9.96\t-9.91\t-9.86\t-10.00\t-9.77\t-9.78\t-9.57\t-9.49\t-9.33\t-9.09\t\n",
      "\n",
      "-9.98\t-9.94\t-9.88\t-9.93\t-9.69\t-9.75\t-9.73\t-9.63\t-9.51\t-9.52\t\n",
      "\n",
      "-9.99\t-9.95\t-9.98\t0.00\t-9.88\t-9.96\t-9.80\t-9.89\t-9.81\t-9.82\t\n",
      "\n",
      "-9.99\t-9.93\t-9.91\t0.00\t-9.79\t-9.93\t-9.83\t-9.83\t-9.89\t-9.82\t\n",
      "\n",
      "-9.99\t-9.88\t-9.77\t0.00\t-10.00\t-10.00\t-9.89\t-9.84\t-9.90\t-9.83\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit(10,500,0.9,wind=[0,0,1,2,1,0])\n",
    "MC_first_visit(10,500,0.9,wind=[0,0,3,3,3,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Första simuleringen (wind=[0,0,1,2,1,0]):\n",
    "GAMMA: 0.9: Värdefunktionen påverkas av vind och gamma. Starkare vind leder till lägre värden, särskilt i områden där vinden är starkare. Systemet tar hänsyn till ökad svårighet att navigera genom vinden.\n",
    "\n",
    "Andra simuleringen (wind=[0,0,3,3,3,0]):\n",
    "GAMMA: 0.9: Påverkan av vind och gamma är liknande den första simuleringen. Starkare vind ger lägre värden, och systemet tar hänsyn till ökad svårighet att röra sig genom de områden där vinden är stark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5ffb9",
   "metadata": {},
   "source": [
    "## Exploring Start Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428bf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_exploring_starts(X =6,Y=1,wind=[0, 0, 1, 2, 1, 0]):\n",
    "    grid = WindyGrid(X ,X, wind)\n",
    "    GAMMA = Y\n",
    "    print(\"GAMMA:\",Y)\n",
    "    print(\"Size:\",X,X)\n",
    "   \n",
    "    # Initialize Q, returns, and pairs visited\n",
    "    Q = {}          \n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "    \n",
    "    # initialize a random policy\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = np.random.choice(grid.possibleActions)\n",
    "    \n",
    "    for i in range(1000000):  \n",
    "        if i % 50000 == 0:\n",
    "            print('starting episode', i)\n",
    "        statesActionsReturns = []\n",
    "        observation = np.random.choice(grid.stateSpace)\n",
    "        action = np.random.choice(grid.possibleActions)\n",
    "        grid.setState(observation)\n",
    "        observation_, reward, done, info = grid.step(action)\n",
    "        memory = [(observation, action, reward)]\n",
    "        steps = 1\n",
    "        while not done:\n",
    "            action = policy[observation_]\n",
    "            steps += 1\n",
    "            observation, reward, done, info = grid.step(action)\n",
    "            if steps > 15 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation_, action, reward))\n",
    "            observation_ = observation\n",
    "\n",
    "        # append the terminal state\n",
    "        memory.append((observation_, action, reward))\n",
    "        \n",
    "        G = 0        \n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False  \n",
    "            else:\n",
    "                statesActionsReturns.append((state,action, G))\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesActionsReturns.reverse()\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])                   \n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.argmax(values)\n",
    "                policy[state] = grid.possibleActions[best]\n",
    "            \n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298a493",
   "metadata": {},
   "source": [
    "## Del 2\n",
    "\n",
    "\n",
    "- Använd  *exploring starts* Monte Carlo Metoden\n",
    "\n",
    "1. Öka vindstyrkan med en enhet.\n",
    "    - Hur ändras slutvärdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur ändras policyn om man ändra gamma till:\n",
    "    - 𝛾=0.5\n",
    "    - 𝛾=0,9\n",
    "    - 𝛾=0,95\n",
    "\n",
    "\n",
    "3. Testa rutnätsvärlden i storlekarna:\n",
    "    - 8x8\n",
    "        - Ändra på vinden, vad händer med policyn?\n",
    "        - Prova med 𝛾=0,9, vad händer med policyn?\n",
    "    - 10x10\n",
    "        - Ändra på vinden, vad händer med policyn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42bdbc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.5\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-1.99913, -1.99917, -1.99914, -1.99808]\t[-1.9982, -1.99856, -1.9992, -1.99613]\t[-1.99651, -1.99627, -1.99841, -1.99222]\t[-1.99235, -1.99316, -1.99623, -1.98438]\t[-1.98448, -1.98508, -1.99262, -1.96875]\t[-1.96899, -1.9375, -1.9844, -1.96921]\t\n",
      "\n",
      "[-1.99912, -1.99909, -1.99908, -1.99826]\t[-1.99807, -1.99815, -1.99987, -1.99612]\t[-1.99625, -1.99638, -1.99814, -1.99219]\t[-1.99612, -1.99219, -1.99617, -1.99622]\t[-1.9844, -1.98445, -1.99221, -1.96878]\t[-1.96891, -1.875, -1.98441, -1.93759]\t\n",
      "\n",
      "[-1.99925, -1.99958, -1.99909, -1.99806]\t[-1.99817, -1.99909, -1.99917, -1.99611]\t[-1.99626, -1.99624, -1.99809, -1.9922]\t[-1.9922, -1.99222, -1.99612, -1.98438]\t[-1.98441, -1.98441, -1.99224, -1.96877]\t[-1.93757, -1.75, -1.9844, -1.87507]\t\n",
      "\n",
      "[-1.99919, -1.9991, -1.99963, -1.99903]\t[-1.99819, -1.99805, -1.99961, -1.99811]\t[-1.99611, -1.99617, -1.99816, -1.99617]\t[-1.99221, -1.9961, -1.99612, -1.98438]\t[-1.9844, -1.98439, -1.99611, -1.93753]\t[-1.87507, -1.5, -1.96877, -1.75003]\t\n",
      "\n",
      "[-1.99956, -1.9991, -1.99907, -1.99806]\t[-1.99911, -1.9981, -1.99908, -1.9961]\t[-1.99617, -1.99808, -1.99814, -1.99219]\t[-1.99221, -1.9922, -1.99611, -1.98439]\t[0, 0, 0, 0]\t[-1.75004, -1.75004, -1.0, -1.50007]\t\n",
      "\n",
      "[-1.99909, -1.9991, -1.99912, -1.99807]\t[-1.99807, -1.99809, -1.99909, -1.9961]\t[-1.99621, -1.99612, -1.99904, -1.99219]\t[-1.99611, -1.9922, -1.99614, -1.9844]\t[-1.98439, -1.0, -1.99221, -1.75004]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tD\tU\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tU\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-6.86559, -6.9545, -6.88067, -6.51575]\t[-6.54046, -6.57462, -6.87498, -6.13013]\t[-6.14206, -6.15158, -6.55681, -5.69825]\t[-5.91878, -5.72308, -6.13525, -5.21719]\t[-5.22133, -5.21992, -5.75819, -4.68566]\t[-4.69611, -4.09511, -5.22702, -4.69436]\t\n",
      "\n",
      "[-6.89114, -6.91691, -6.93978, -6.53037]\t[-6.53273, -6.55504, -6.9856, -6.15218]\t[-6.16804, -6.13857, -6.53659, -5.69758]\t[-6.15927, -5.69718, -6.14152, -6.13123]\t[-5.2319, -5.22255, -5.70828, -4.68618]\t[-4.68985, -3.43908, -5.22772, -4.09974]\t\n",
      "\n",
      "[-6.94789, -7.20473, -6.88829, -6.54231]\t[-6.61279, -6.88785, -6.90247, -6.14024]\t[-6.12864, -6.16999, -6.53348, -5.69669]\t[-5.70461, -5.70983, -6.13332, -5.21736]\t[-5.22925, -5.22665, -5.70433, -4.68594]\t[-4.09982, -2.71003, -5.22129, -3.44064]\t\n",
      "\n",
      "[-6.8885, -6.86932, -7.20936, -6.8811]\t[-6.58632, -6.51675, -7.18702, -6.54824]\t[-6.13054, -6.16606, -6.54307, -6.15296]\t[-5.70312, -6.13274, -6.14275, -5.21809]\t[-5.22517, -5.22268, -6.13074, -4.09731]\t[-3.44226, -1.90001, -4.68992, -2.71176]\t\n",
      "\n",
      "[-7.18805, -6.89986, -6.87279, -6.52421]\t[-6.89094, -6.52795, -6.87989, -6.13523]\t[-6.22407, -6.52504, -6.53957, -5.69592]\t[-5.70165, -5.70032, -6.15701, -5.21703]\t[0, 0, 0, 0]\t[-2.71172, -2.71165, -1.0, -1.9037]\t\n",
      "\n",
      "[-6.89158, -6.89835, -6.91048, -6.52034]\t[-6.5416, -6.54082, -6.91159, -6.12862]\t[-6.14595, -6.1401, -6.87763, -5.69936]\t[-6.13054, -5.71094, -6.14447, -5.21987]\t[-5.2198, -1.0, -5.71781, -2.71169]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "D\tD\tU\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tU\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.95\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-8.68873, -8.65028, -8.69519, -8.03353]\t[-8.06377, -8.05862, -8.67423, -7.39765]\t[-7.47236, -7.53576, -8.10212, -6.73245]\t[-6.74532, -6.7455, -7.51956, -6.03389]\t[-6.08072, -6.04167, -6.76405, -5.29835]\t[-5.31844, -4.5246, -6.03886, -5.31892]\t\n",
      "\n",
      "[-8.7028, -8.65895, -8.65861, -8.02625]\t[-8.05673, -8.04906, -8.68665, -7.3953]\t[-7.40561, -7.41237, -8.05812, -6.73397]\t[-7.40256, -6.73329, -7.41053, -7.40529]\t[-6.06589, -6.04996, -6.75541, -5.29987]\t[-5.30983, -3.71, -6.05825, -4.5361]\t\n",
      "\n",
      "[-8.63755, -9.2214, -8.67154, -8.03848]\t[-8.06557, -8.64298, -8.66355, -7.39559]\t[-7.42054, -7.40277, -8.06059, -6.73201]\t[-6.73432, -6.74453, -7.41385, -6.03342]\t[-6.04136, -6.04427, -6.7369, -5.2993]\t[-4.54846, -2.85253, -6.04013, -3.71908]\t\n",
      "\n",
      "[-8.67721, -8.62549, -9.19757, -8.64526]\t[-8.03991, -8.03738, -9.20909, -8.0354]\t[-7.43015, -7.40037, -8.02981, -7.39573]\t[-6.73693, -7.40272, -7.41547, -6.03465]\t[-6.03883, -6.0383, -7.39752, -4.52585]\t[-3.72199, -1.95005, -5.30371, -2.85574]\t\n",
      "\n",
      "[-9.22102, -8.65968, -8.63377, -8.02637]\t[-8.63275, -8.06842, -8.64533, -7.39588]\t[-7.41308, -8.03229, -8.03999, -6.73216]\t[-6.73949, -6.73684, -7.40263, -6.03868]\t[0, 0, 0, 0]\t[-2.85569, -2.859, -1.0, -1.95334]\t\n",
      "\n",
      "[-8.65504, -8.65741, -8.64416, -8.04183]\t[-8.03767, -8.06815, -8.65119, -7.41124]\t[-7.39985, -7.4026, -8.64084, -6.73307]\t[-7.40755, -6.73423, -7.39765, -6.045]\t[-6.04299, -1.0, -6.73437, -2.8525]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "D\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts(6,0.5,wind=[0, 0, 2, 3, 2, 0])\n",
    "MC_exploring_starts(6,0.9,wind=[0, 0, 2, 3, 2, 0])\n",
    "MC_exploring_starts(6,0.95,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMMA: 0.5\n",
    "Slutvärdesfunktionen verkar minska gradvis från det högsta värdet längst upp till vänster till det lägsta värdet längst ner till höger.\n",
    "\n",
    "GAMMA: 0.9\n",
    "Slutvärdesfunktionen verkar vara mindre benägen att minska snabbt jämfört med gamma 0.5. Det finns en ökad tendens till att höga värden sprider sig över området.\n",
    "\n",
    "GAMMA: 0.95\n",
    "Slutvärdesfunktionen verkar ha en ännu mindre benägenhet att minska snabbt. Det finns en ökad utjämning av höga värden över hela området."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7f92dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 8 8\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-6.63444, -6.56437, -6.63036, -6.15865]\t[-6.15302, -6.23117, -6.73056, -5.70572]\t[-5.80481, -5.71858, -6.19972, -5.22225]\t[-5.35982, -5.26968, -5.73351, -4.68801]\t[-4.74432, -4.70543, -5.22317, -4.09857]\t[-4.1036, -3.44216, -4.69033, -4.72314]\t[-4.6955, -4.09546, -4.13332, -5.22421]\t[-5.22254, -4.70123, -4.68881, -5.22402]\t\n",
      "\n",
      "[-6.7038, -6.56719, -6.57393, -6.15349]\t[-6.17294, -6.18371, -6.61247, -5.69768]\t[-5.7287, -5.727, -6.19422, -5.21855]\t[-5.74815, -5.22531, -5.70804, -5.71454]\t[-4.69788, -4.77646, -5.22995, -4.09568]\t[-4.10183, -2.71349, -4.69937, -4.10216]\t[-4.6903, -3.43917, -3.45096, -4.69321]\t[-5.23107, -4.09801, -4.09569, -4.69646]\t\n",
      "\n",
      "[-6.55935, -6.88218, -6.57581, -6.18306]\t[-6.27097, -6.53254, -6.57003, -5.70226]\t[-5.72909, -5.7374, -6.14707, -5.21928]\t[-5.22707, -5.23584, -5.71958, -4.6886]\t[-4.70354, -4.81203, -5.23931, -4.09583]\t[-3.44651, -1.90004, -4.70102, -3.46419]\t[-4.09796, -2.71192, -2.71735, -4.09793]\t[-4.69081, -3.43962, -3.44209, -4.10308]\t\n",
      "\n",
      "[-6.61375, -6.51472, -6.87858, -6.53266]\t[-6.19163, -6.13061, -6.88955, -6.16695]\t[-5.70072, -5.72903, -6.1586, -5.75039]\t[-5.23352, -5.70579, -5.70741, -4.68802]\t[0, 0, 0, 0]\t[-2.71653, -2.71454, -1.0, -2.71329]\t[-3.4478, -3.44178, -1.90046, -3.44195]\t[-4.09782, -4.10085, -2.71034, -3.44219]\t\n",
      "\n",
      "[-6.87836, -6.54795, -6.52151, -6.12723]\t[-6.54474, -6.15069, -6.53932, -5.698]\t[-5.70519, -6.13918, -6.19307, -5.2196]\t[-5.2357, -5.23696, -5.71985, -4.68778]\t[-4.69802, -1.0, -5.2254, -2.71662]\t[-1.90339, -3.44465, -1.9007, -3.44194]\t[-2.71023, -4.09833, -2.71459, -4.10052]\t[-3.44192, -4.69088, -3.43936, -4.09803]\t\n",
      "\n",
      "[-6.52355, -6.55246, -6.5307, -6.14487]\t[-6.1427, -6.20548, -6.5371, -5.69705]\t[-5.71962, -5.70633, -6.53832, -5.2193]\t[-5.711, -5.22689, -5.72669, -4.70444]\t[-4.70739, -1.9034, -5.22103, -1.9]\t[-2.71511, -2.71035, -2.71317, -4.09796]\t[-3.4426, -3.44207, -3.439, -4.68855]\t[-4.09566, -4.10055, -4.09842, -4.69072]\t\n",
      "\n",
      "[-6.53244, -6.54562, -6.57028, -6.1343]\t[-6.1615, -6.19592, -6.5554, -5.70807]\t[-6.14352, -5.70637, -6.14555, -5.21968]\t[-5.22185, -5.23238, -6.13192, -1.0]\t[-1.0, -2.71315, -5.22807, -2.71385]\t[-3.44178, -3.44319, -1.90012, -3.44187]\t[-4.09804, -4.09827, -2.71018, -4.09793]\t[-4.68804, -4.69078, -3.439, -4.09785]\t\n",
      "\n",
      "[-6.5687, -6.5399, -6.53627, -6.14072]\t[-6.16276, -6.15316, -6.54088, -5.71261]\t[-5.70441, -5.72665, -6.14367, -5.23478]\t[-5.21891, -5.23148, -5.69953, -1.90325]\t[-1.9, -1.9065, -5.23231, -3.439]\t[-2.71083, -3.44282, -2.71343, -4.0951]\t[-3.4396, -4.10359, -3.44328, -4.68826]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\tD\tL\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\tD\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\tD\tD\t\n",
      "\n",
      "D\tD\tU\tR\t\tL\tL\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tL\tU\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\tL\tU\t\n",
      "\n",
      "R\tR\tR\tR\tU\tL\tL\tL\t\n",
      "\n",
      "R\tR\tR\tR\tU\tU\tU\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts(8,0.9,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcd0efc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 10 10\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-7.19725, -6.98567, -7.25034, -6.67004]\t[-6.7863, -6.88453, -7.26878, -6.18669]\t[-6.40165, -6.62071, -7.15188, -5.71547]\t[-5.75464, -5.89609, -6.76459, -5.21744]\t[-5.47549, -5.23362, -5.76989, -4.69355]\t[-4.73632, -4.10172, -5.27879, -4.12004]\t[-4.10926, -3.45081, -4.72296, -3.44288]\t[-3.44419, -2.71055, -4.10499, -2.76407]\t[-2.72078, -1.90122, -3.47712, -3.46308]\t[-3.45787, -2.712, -2.72561, -3.44871]\t\n",
      "\n",
      "[-7.23241, -6.9199, -6.95729, -6.62303]\t[-6.76124, -6.68341, -6.97416, -6.13853]\t[-6.29119, -6.39148, -6.97002, -5.70333]\t[-6.20041, -5.70839, -6.15896, -6.15393]\t[-5.2261, -5.26754, -5.76655, -4.68581]\t[-4.71248, -3.44595, -5.23723, -3.45395]\t[-4.10519, -2.72566, -4.10914, -2.71322]\t[-3.44872, -1.90674, -3.44842, -1.90065]\t[-2.72478, -1.0, -2.71526, -2.71469]\t[-3.45696, -1.90591, -1.90059, -2.71484]\t\n",
      "\n",
      "[-6.96079, -7.6426, -6.92778, -6.53926]\t[-6.63282, -6.88203, -7.36863, -6.12711]\t[-6.36428, -6.23623, -6.87142, -5.69685]\t[-5.71828, -5.70685, -6.36348, -5.26302]\t[-5.22961, -5.24194, -5.7443, -4.68602]\t[-4.16323, -4.116, -5.24145, -2.71533]\t[-3.45385, -3.48559, -3.45, -1.90265]\t[-2.71595, -2.7201, -2.71484, -1.0]\t[0, 0, 0, 0]\t[-2.71557, -2.71493, -1.0, -1.90506]\t\n",
      "\n",
      "[-6.89114, -6.9293, -7.24146, -7.35177]\t[-6.56991, -6.74701, -7.20775, -6.51767]\t[-6.38075, -6.12867, -6.68444, -6.14866]\t[-5.70031, -6.13633, -6.37279, -5.22054]\t[-5.2377, -5.22872, -6.15066, -4.09904]\t[-3.46226, -4.69831, -4.71899, -3.44194]\t[-2.75591, -4.1143, -4.12382, -2.71032]\t[-1.9063, -3.45383, -3.45872, -1.9002]\t[-1.0, -2.71979, -2.7156, -2.71563]\t[-1.90563, -3.44472, -1.9, -2.71487]\t\n",
      "\n",
      "[-7.2592, -6.93736, -6.93874, -6.52445]\t[-6.88066, -6.57945, -6.97244, -6.19702]\t[-6.32616, -6.55967, -6.54885, -5.74818]\t[-5.72484, -5.73382, -6.23058, -5.2181]\t[-5.24057, -4.71132, -5.70768, -3.43925]\t[-4.11085, -5.25294, -4.10041, -4.09678]\t[-3.44167, -4.69007, -4.69825, -3.45298]\t[-2.71064, -4.09918, -4.09937, -2.71591]\t[-1.9002, -3.46108, -3.4456, -3.44788]\t[-2.71019, -4.10114, -2.71755, -3.44378]\t\n",
      "\n",
      "[-6.9323, -6.94067, -6.91043, -6.56494]\t[-6.54191, -6.57943, -6.9326, -6.17802]\t[-6.17048, -6.14758, -6.87013, -5.73385]\t[-6.14065, -5.74916, -6.19333, -5.2176]\t[-5.22936, -4.09636, -5.76228, -4.11288]\t[-4.70208, -5.69961, -4.68818, -4.72522]\t[-4.1082, -5.23309, -5.24961, -4.09596]\t[-3.43939, -4.69301, -4.6989, -3.45866]\t[-2.71065, -4.14628, -4.09921, -4.12673]\t[-3.43931, -4.69444, -3.45701, -4.12512]\t\n",
      "\n",
      "[-6.91237, -6.86561, -6.94905, -6.52878]\t[-6.58254, -6.56694, -6.95406, -6.13041]\t[-6.54048, -6.20765, -6.54188, -5.70164]\t[-5.70344, -5.73214, -6.53854, -4.68628]\t[-4.68643, -4.70367, -5.71623, -4.69118]\t[-5.24726, -5.22752, -5.23336, -5.21785]\t[-4.68656, -5.70003, -5.70726, -4.69095]\t[-4.09555, -5.23045, -5.22169, -4.12992]\t[-3.45378, -4.72426, -4.70769, -4.69383]\t[-4.09539, -5.22687, -4.2368, -4.69406]\t\n",
      "\n",
      "[-6.92797, -6.53418, -6.88566, -6.51671]\t[-6.58261, -6.12735, -6.87875, -6.15012]\t[-6.13268, -6.15983, -6.58849, -5.70163]\t[-5.7587, -5.69924, -6.14744, -4.09558]\t[-4.09705, -5.22438, -5.71047, -5.23311]\t[-5.70247, -5.72014, -4.68725, -5.7035]\t[-5.21915, -6.14093, -5.23104, -5.23284]\t[-4.68825, -5.71959, -5.70324, -4.70713]\t[-4.10026, -5.25534, -5.22651, -5.23003]\t[-4.69456, -5.70879, -4.70076, -5.23397]\t\n",
      "\n",
      "[-6.8745, -6.12938, -6.53869, -6.14743]\t[-6.52116, -5.69774, -6.53409, -5.71489]\t[-6.19059, -6.16708, -6.54859, -5.22369]\t[-5.71535, -5.21703, -6.18591, -4.68559]\t[-4.68766, -4.69828, -5.22815, -5.6994]\t[-5.23404, -5.70841, -5.22111, -6.12958]\t[-5.69749, -6.15191, -5.71365, -5.71255]\t[-5.22117, -6.14829, -6.12954, -5.30824]\t[-4.68792, -5.71338, -5.72356, -5.74515]\t[-5.22673, -6.13357, -5.24699, -5.70851]\t\n",
      "\n",
      "[-6.54268, -6.14322, -6.13014, -5.69659]\t[-6.15458, -5.70284, -6.12993, -5.21802]\t[-6.15165, -5.70283, -6.51665, -4.68642]\t[-5.70236, -4.68947, -6.14311, -5.21703]\t[-5.2241, -5.21703, -4.68649, -5.2291]\t[-5.71928, -5.70981, -5.22002, -6.15241]\t[-6.13159, -6.14859, -5.69929, -6.14881]\t[-5.72025, -6.16656, -6.18173, -5.70126]\t[-5.22858, -5.72054, -6.16082, -6.13559]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\tR\tD\tD\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\tR\tR\tD\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tR\tR\t\tL\t\n",
      "\n",
      "U\tR\tD\tR\tR\tR\tR\tR\tU\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tU\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tR\tD\tL\tR\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tR\tU\tR\tU\tU\tU\tU\t\n",
      "\n",
      "R\tD\tR\tR\tU\tL\tU\tU\tU\tU\t\n",
      "\n",
      "D\tD\tR\tR\tU\tL\tU\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tD\tL\tL\tL\tR\tU\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts(10,0.9,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analys av 8x8:\n",
    "Observationer:\n",
    "Generellt sett verkar policyn anpassa sig till den givna vinden (wind) och försöker hitta en väg till målet.\n",
    "I vissa områden där vinden är stark, kan policyn välja att gå neråt (D) för att dra nytta av vinden.\n",
    "Policyn verkar vara känslig för både hinder och vindens påverkan, vilket kan leda till alternativa vägar för att nå målet.\n",
    "\n",
    "Analys av 10x10:\n",
    "Observationer:\n",
    "Mönstret är liknande 8x8, men på grund av en större miljö har policyn mer utrymme att anpassa sig och hitta effektivare vägar.\n",
    "Policyns beteende verkar vara mer robust i större miljöer och kan hantera de långa vindsträckorna bättre.\n",
    "\n",
    "Sammanfattning:\n",
    "Policyn tycks anpassa sig väl till både vind och hinder för att nå målet.\n",
    "Större miljöer tillåter mer flexibilitet och bättre anpassning till vindförhållandena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b42a6",
   "metadata": {},
   "source": [
    "## On-policy first visit Monte Carlo for $\\varepsilon$-soft policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba2dcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_without_exploring_starts(X =6,Y=0.9):\n",
    "    grid = WindyGrid(X,X, wind=[0, 0, 1, 2, 1, 0])\n",
    "    GAMMA = Y\n",
    "    EPS = 0.4\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.actionSpace.keys():\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    for i in range(1000000):\n",
    "        statesActionsReturns = []\n",
    "        if i % 100000 == 0:\n",
    "            print('starting episode', i)\n",
    "        observation, done = grid.reset()       \n",
    "        memory = []\n",
    "        steps = 0\n",
    "        while not done:       \n",
    "            if len(policy[observation]) > 1:\n",
    "                action = np.random.choice(policy[observation])\n",
    "            else:\n",
    "                action = policy[observation]\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        #append the terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0        \n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):                                    \n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action,G))           \n",
    "            G = GAMMA*G + reward\n",
    "        statesActionsReturns.reverse()\n",
    "\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])                   \n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.random.choice(np.where(values==values.max())[0])                    \n",
    "                rand = np.random.random()\n",
    "                if rand < 1 - EPS:\n",
    "                    policy[state] = grid.possibleActions[best]\n",
    "                else:                        \n",
    "                    policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88293b06",
   "metadata": {},
   "source": [
    "## Del 3\n",
    "- Använd *without exploring starts* Monte Carlo Metoden\n",
    "\n",
    "1. Öka vindstyrkan med en enhet.\n",
    "    - Hur ändras slutvärdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur ändras policyn om man ändra gamma till:\n",
    "    - 𝛾=0.5\n",
    "    - 𝛾=0,9\n",
    "    - 𝛾=0,95\n",
    "\n",
    "\n",
    "3. Testa rutnätsvärlden i storlekarna:\n",
    "    - 8x8\n",
    "        - Ändra på vinden, vad händer med policyn?\n",
    "        - Prova med 𝛾=0,9, vad händer med policyn?\n",
    "    - 10x10\n",
    "        - Ändra på vinden, vad händer med policyn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2dc828a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_without_exploring_starts(6,0.5)\n",
    "# MC_without_exploring_starts(6,0.9)\n",
    "# MC_without_exploring_starts(6,0.95)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1a3d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_without_exploring_starts(8,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0509f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_without_exploring_starts(10,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a2a0d",
   "metadata": {},
   "source": [
    "## Off-Policy Monte Carlo prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc7f57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_off_policy_prediction(X =6,Y=0.9):\n",
    "    grid = WindyGrid(X,X, wind=[0,0,1,2,1,0])\n",
    "    GAMMA = Y\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state,action)] = 0\n",
    "            C[(state,action)] = 0\n",
    "    \n",
    "    targetPolicy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        targetPolicy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)            \n",
    "        behaviorPolicy = {}\n",
    "        for state in grid.stateSpace:\n",
    "            behaviorPolicy[state] = grid.possibleActions\n",
    "        memory = []\n",
    "        observation, done = grid.reset()\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(behaviorPolicy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "        memory.append((observation, action, reward))\n",
    "        \n",
    "        G = 0\n",
    "        W = 1\n",
    "        last = True\n",
    "        for (state, action, reward) in reversed(memory):            \n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                C[state,action] += W\n",
    "                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n",
    "                prob = 1 if action in targetPolicy[state] else 0\n",
    "                W *= prob/(1/len(behaviorPolicy[state]))\n",
    "                if W == 0:\n",
    "                    break\n",
    "            G = GAMMA*G + reward\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(targetPolicy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b9cc6",
   "metadata": {},
   "source": [
    "## Del 4\n",
    "- Använd *off-policy prediction* Monte Carlo Metoden\n",
    "\n",
    "1. Öka vindstyrkan med en enhet.\n",
    "    - Hur ändras slutvärdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur ändras policyn om man ändra gamma till:\n",
    "    - 𝛾=0.5\n",
    "    - 𝛾=0,9\n",
    "    - 𝛾=0,95\n",
    "\n",
    "\n",
    "3. Testa rutnätsvärlden i storlekarna:\n",
    "    - 8x8\n",
    "        - Ändra på vinden, vad händer med policyn?\n",
    "        - Prova med 𝛾=0,9, vad händer med policyn?\n",
    "    - 10x10\n",
    "        - Ändra på vinden, vad händer med policyn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "406a1b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_off_policy_prediction(6,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "560a06c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_off_policy_prediction(8,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "455c795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MC_off_policy_prediction(10,0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
