{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2851194",
   "metadata": {},
   "source": [
    "## GridWorld\n",
    "Ph.D Leonarod A, Espinosa, M.Sc Andrej Scherbakov-Parland, BIT Kristoffer Kuvaja Adolfsson\n",
    "\n",
    "### Bibliography:\n",
    "\n",
    "* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n",
    "http://incompleteideas.net/book/bookdraft2017nov5.pdf  (chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1f756d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d07eafa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilits\n",
    "def printV(V, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy \n",
    "            print('%.2f' % V[state], end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')\n",
    "\n",
    "def printPolicy(policy, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy \n",
    "            if state in grid.stateSpace:\n",
    "                string = ''.join(policy[state])\n",
    "                print(string, end='\\t')\n",
    "            else:\n",
    "                print('', end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')    \n",
    "\n",
    "def printQ(Q, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy            \n",
    "            if state != grid.m * grid.n - 1:\n",
    "                vals = [np.round(Q[state,action], 5) for action in grid.possibleActions]\n",
    "                print(vals, end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')\n",
    "\n",
    "def sampleReducedActionSpace(grid, action):\n",
    "    actions = grid.possibleActions[:]\n",
    "    actions.remove(action)\n",
    "    sample = np.random.choice(actions)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ffbb837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindyGrid(object):\n",
    "    def __init__(self, m, n, wind):         \n",
    "        self.grid = np.zeros((m,n))                            # representation of the grid\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.stateSpace = [i for i in range(self.m*self.n)]        \n",
    "        self.stateSpace.remove(28)                              # Terminal state\n",
    "        self.stateSpacePlus = [i for i in range(self.m*self.n)] # State space + terminal state\n",
    "        self.actionSpace = {'U': -self.m, 'D': self.m, \n",
    "                            'L': -1, 'R': 1}\n",
    "        self.possibleActions = ['U', 'D', 'L', 'R']\n",
    "        self.agentPosition = 0\n",
    "        self.wind = wind\n",
    "\n",
    "    def isTerminalState(self, state):\n",
    "        return state in self.stateSpacePlus and state not in self.stateSpace \n",
    "\n",
    "    def getAgentRowAndColumn(self):                               # position of agent\n",
    "        x = self.agentPosition // self.m\n",
    "        y = self.agentPosition % self.n\n",
    "        return x, y\n",
    "    \n",
    "    def setState(self, state):\n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 0            \n",
    "        self.agentPosition = state        \n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 1   \n",
    "    \n",
    "    def offGridMove(self, newState, oldState):\n",
    "        # if we move into a row not in the grid\n",
    "        if newState not in self.stateSpacePlus:\n",
    "            return True\n",
    "        # if we're trying to wrap around to next row\n",
    "        elif oldState % self.m == 0 and newState  % self.m == self.m - 1:\n",
    "            return True\n",
    "        elif oldState % self.m == self.m - 1 and newState % self.m == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False   \n",
    "        \n",
    "    # Include wind stenght.\n",
    "    def step22(self, action):\n",
    "        agentX, agentY = self.getAgentRowAndColumn()\n",
    "        if agentY >= 0 and agentY < len(self.wind):  # Kontrollera om agentY 칛r ett giltigt index\n",
    "            resultingState = self.agentPosition + self.actionSpace[action] + self.wind[agentY] * self.actionSpace['U']\n",
    "            if resultingState < 0:  # If the wind is trying to push the agent off the grid\n",
    "                resultingState += self.m\n",
    "\n",
    "        if agentX > 0:\n",
    "            resultingState = self.agentPosition + self.actionSpace[action] + \\\n",
    "                            self.wind[agentY] * self.actionSpace['U']\n",
    "            if resultingState < 0: #if the wind is trying to push agent off grid\n",
    "                resultingState += self.m\n",
    "        else:\n",
    "            if action == 'L' or action == 'R':\n",
    "                resultingState = self.agentPosition + self.actionSpace[action]\n",
    "            else:\n",
    "                resultingState = self.agentPosition + self.actionSpace[action] + \\\n",
    "                            self.wind[agentY] * self.actionSpace['U']\n",
    "        #reward = -1 if not self.isTerminalState(resultingState) else 0\n",
    "        reward = -1\n",
    "        if not self.offGridMove(resultingState, self.agentPosition):\n",
    "            self.setState(resultingState)\n",
    "            return resultingState, reward, self.isTerminalState(resultingState), None\n",
    "        else:\n",
    "            return self.agentPosition, reward, self.isTerminalState(self.agentPosition), None\n",
    "        \n",
    "    def step(self, action):\n",
    "        agentX, agentY = self.getAgentRowAndColumn()\n",
    "\n",
    "        if agentY >= 0 and agentY < len(self.wind):  # Kontrollera om agentY 칛r ett giltigt index\n",
    "            wind_effect = self.wind[agentY] * self.actionSpace['U']\n",
    "        else:\n",
    "            wind_effect = 0\n",
    "\n",
    "        if agentX > 0:\n",
    "            resultingState = self.agentPosition + self.actionSpace[action] + wind_effect\n",
    "        else:\n",
    "            if action == 'L' or action == 'R':\n",
    "                resultingState = self.agentPosition + self.actionSpace[action]\n",
    "            else:\n",
    "                resultingState = self.agentPosition + self.actionSpace[action] + wind_effect\n",
    "\n",
    "        if resultingState < 0:  # If the wind or action is trying to push the agent off the grid\n",
    "            resultingState += self.m\n",
    "\n",
    "        reward = -1\n",
    "\n",
    "        if not self.offGridMove(resultingState, self.agentPosition):\n",
    "            self.setState(resultingState)\n",
    "            return resultingState, reward, self.isTerminalState(resultingState), None\n",
    "        else:\n",
    "            return self.agentPosition, reward, self.isTerminalState(self.agentPosition), None\n",
    "\n",
    "    def reset(self):\n",
    "        self.agentPosition = 0\n",
    "        self.grid = np.zeros((self.m,self.n))\n",
    "        return self.agentPosition, False\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.grid:\n",
    "            for col in row:\n",
    "                if col == 0:\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 1:\n",
    "                    print('X', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216e193",
   "metadata": {},
   "source": [
    "## First visit Monte Carlo Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2387e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_first_visit(X =6 ,loop=500,Y=1,  wind=[0, 0, 1, 2, 1, 0]):\n",
    "    print(\"GAMMA:\",Y)\n",
    "    print(\"Size:\",X,X)\n",
    "\n",
    "    grid = WindyGrid(X,X, wind)\n",
    "    GAMMA = 1.0\n",
    "    if(Y):\n",
    "        GAMMA = Y\n",
    "    \n",
    "\n",
    "    policy = {}                              #  a dictionary that maps each\n",
    "    for state in grid.stateSpace:            #  state to the list of possible actions\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    V = {}                                   # Initialize our initial estimate of the value\n",
    "    for state in grid.stateSpacePlus:        # function. Each state gets a value of 0.\n",
    "        V[state] = 0                                                              \n",
    "\n",
    "    returns = {}                             #Initialize a dictionary that keeps a list\n",
    "    for state in grid.stateSpace:            #of the returns for each state.\n",
    "        returns[state] = []\n",
    "\n",
    "    for i in range(loop):                     # Loop over 500 games,  \n",
    "        observation, done = grid.reset()     # resetting the grid and memory with each game.\n",
    "        memory = []                          # empty list to keep track of the states visited \n",
    "        statesReturns = []                   # and returns at each time step\n",
    "        if i % 100 == 0:                     # Just to know if the game is running.\n",
    "            print('starting episode', i)\n",
    "        while not done:                      # While the game isn't done \n",
    "            # attempt to follow the policy. In this case choose an action \n",
    "            # according to the random equiprobable strategy.\n",
    "            action = np.random.choice(policy[observation])    \n",
    "            observation_, reward, done, info = grid.step(action)  # Take that action, get new state, reward and done\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        # append terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0                                  # set G=0\n",
    "        last = True                            # initialize a Boolean to keep track of the visit to the last state                   \n",
    "        for state, action, reward in reversed(memory): \n",
    "            if last:\n",
    "                last = False\n",
    "            else:                                    # Skip the terminal state and append the set of states\n",
    "                statesReturns.append((state,G))      #  and returns to the statesReturns list. \n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesReturns.reverse()                  # to ge it in chronological order\n",
    "        statesVisited = []                       # keep track of the visited states during the episode.\n",
    "        for state, G in statesReturns:\n",
    "            if state not in statesVisited:       # Iterate over the episode and see \n",
    "                returns[state].append(G)         # if each state has been visited before. \n",
    "                V[state] = np.mean(returns[state]) \n",
    "                statesVisited.append(state)\n",
    "                \n",
    "                #If it hasn't, meaning this is the agent's first visit, go ahead and append \n",
    "                #the returns to the returns dictionary for that state.\n",
    "                #Calculate the value function by taking the mean of the returns for that state, and finally, \n",
    "                #append that state to the list of statesVisited. \n",
    "    print(\"\\n\") \n",
    "    printV(V, grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c0c1eb",
   "metadata": {},
   "source": [
    "## Del 1:\n",
    "\n",
    "- Anv칛nd *first visit* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur 칛ndras v칛rdefunktion om man 칛ndra gamma till:\n",
    "    - 洧=0.5\n",
    "    - 洧=0,9\n",
    "    - 洧=0,95\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med v칛rdefunktion?\n",
    "        - Prova med 洧=0,9, vad h칛nder med v칛rdefunktion?\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med v칛rdefunktion?\n",
    "        - Prova med 洧=0,9, vad h칛nder med v칛rdefunktion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2885845a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.5\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.99\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t-2.00\t-1.95\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t-2.00\t0.00\t-1.74\t\n",
      "\n",
      "-2.00\t-2.00\t-2.00\t0.00\t-1.96\t-1.94\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.99\t-9.98\t-9.99\t-9.97\t-9.96\t-9.92\t\n",
      "\n",
      "-10.00\t-9.99\t-9.98\t-9.98\t-9.94\t-9.78\t\n",
      "\n",
      "-9.99\t-10.00\t-9.98\t-9.99\t-9.91\t-9.43\t\n",
      "\n",
      "-9.99\t-10.00\t-9.99\t-9.97\t-9.89\t-8.43\t\n",
      "\n",
      "-10.00\t-10.00\t-10.00\t-9.95\t0.00\t-6.02\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t0.00\t-8.59\t-8.03\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.95\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-19.95\t-19.94\t-19.90\t-19.86\t-19.74\t-19.62\t\n",
      "\n",
      "-19.94\t-19.92\t-19.91\t-19.83\t-19.74\t-19.42\t\n",
      "\n",
      "-19.94\t-19.90\t-19.86\t-19.87\t-19.71\t-18.78\t\n",
      "\n",
      "-19.94\t-19.91\t-19.90\t-19.74\t-19.53\t-16.64\t\n",
      "\n",
      "-19.94\t-19.92\t-19.84\t-19.83\t0.00\t-11.16\t\n",
      "\n",
      "-19.91\t-19.87\t-19.90\t0.00\t-16.77\t-14.64\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit(6,500,0.5)\n",
    "MC_first_visit(6,500,0.9)\n",
    "MC_first_visit(6,500,0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H칛r 칛r observationer:\n",
    "\n",
    "- 洧 = 0.5:\n",
    "V칛rdefunktionen verkar inte ge mycket vikt 친t framtida bel칬ningar. Det 칛r tydligt eftersom v칛rdena i den nedre h칬gra delen av rutn칛tet 칛r fortfarande ganska l친ga, 칛ven n칛r det finns en positiv bel칬ning d칛r.\n",
    "\n",
    "- 洧 = 0.9:\n",
    "H칛r ser det ut som att systemet ger mer vikt 친t framtida bel칬ningar. V칛rdena i den nedre h칬gra delen av rutn칛tet 칛r l칛gre 칛n i det f칬rsta fallet, vilket tyder p친 att systemet tar h칛nsyn till de l친ngsiktiga konsekvenserna.\n",
    "\n",
    "- 洧 = 0.95:\n",
    "Detta scenario verkar ge 칛nnu mer vikt 친t framtida bel칬ningar. V칛rdena i den nedre h칬gra delen av rutn칛tet 칛r 칛nnu l칛gre, och systemet verkar vara mer inriktat p친 att maximera de l친ngsiktiga bel칬ningarna.\n",
    "\n",
    "Sammanfattningsvis kan du s칛ga att med 칬kande v칛rden p친 gamma ger systemet mer vikt 친t framtida bel칬ningar och blir mer inriktat p친 att maximera de l친ngsiktiga bel칬ningarna j칛mf칬rt med omedelbara bel칬ningar. Detta 칛r en typisk observation i f칬rst칛rkningsinl칛rning, d칛r valet av gamma p친verkar agentens inl칛rningsbeteende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 8 8\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.98\t-9.97\t-9.96\t-9.91\t-9.86\t-9.77\t-9.81\t-9.75\t\n",
      "\n",
      "-9.98\t-9.98\t-9.96\t-9.93\t-9.88\t-9.60\t-9.60\t-9.64\t\n",
      "\n",
      "-9.97\t-9.97\t-9.97\t-9.95\t-9.86\t-9.10\t-9.32\t-9.49\t\n",
      "\n",
      "-9.96\t-9.95\t-9.95\t-9.96\t0.00\t-6.69\t-8.77\t-9.39\t\n",
      "\n",
      "-9.94\t-9.92\t-9.92\t-9.97\t-8.91\t-8.58\t-9.17\t-9.39\t\n",
      "\n",
      "-9.90\t-9.86\t-9.98\t-6.81\t-7.12\t-8.73\t-9.28\t-9.57\t\n",
      "\n",
      "-9.77\t-9.63\t-9.15\t-9.36\t-8.19\t-9.11\t-9.41\t-9.62\t\n",
      "\n",
      "-9.89\t-9.83\t-9.68\t0.00\t-8.91\t-9.35\t-9.53\t-9.68\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 8 8\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.99\t-9.98\t-9.98\t-9.96\t-9.92\t-9.84\t-9.78\t-9.81\t\n",
      "\n",
      "-9.99\t-9.98\t-9.98\t-9.96\t-9.87\t-9.66\t-9.63\t-9.66\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t-9.93\t-9.89\t-8.92\t-9.28\t-9.48\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t-9.96\t0.00\t-6.97\t-8.81\t-9.33\t\n",
      "\n",
      "-9.99\t-9.97\t-9.94\t-9.98\t-7.04\t-8.29\t-9.22\t-9.37\t\n",
      "\n",
      "-9.99\t-9.99\t-9.96\t-9.99\t-9.01\t-9.02\t-9.25\t-9.45\t\n",
      "\n",
      "-9.99\t-9.98\t-9.95\t0.00\t-7.85\t-8.91\t-9.35\t-9.55\t\n",
      "\n",
      "-9.99\t-9.99\t-9.98\t0.00\t-8.64\t-9.18\t-9.51\t-9.71\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit(8,500,0.9)\n",
    "MC_first_visit(8,500,0.9,wind=[0,0,2,3,2,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F칬rsta simuleringen:\n",
    "GAMMA: 0.9, Wind=[0,0,1,2,1,0]: V칛rdefunktionen p친verkas av vind och gamma. Starkare vind leder till l칛gre v칛rden, och systemet tar h칛nsyn till sv친righeten att r칬ra sig i den riktningen.\n",
    "\n",
    "Andra simuleringen:\n",
    "GAMMA: 0.9, Wind=[0,0,2,3,2,0]: Liknande p친verkan som i den f칬rsta simuleringen. Starkare vind ger l칛gre v칛rden, och systemet tar h칛nsyn till 칬kad sv친righet att navigera genom vinden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb5c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 10 10\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.97\t-9.95\t-9.91\t-9.86\t-9.75\t-9.48\t-9.07\t-8.38\t-8.04\t-8.03\t\n",
      "\n",
      "-9.97\t-9.96\t-9.93\t-9.83\t-9.72\t-9.45\t-8.74\t-7.64\t-6.34\t-7.03\t\n",
      "\n",
      "-9.98\t-9.95\t-9.93\t-9.83\t-9.66\t-9.31\t-8.62\t-6.55\t0.00\t-5.47\t\n",
      "\n",
      "-9.97\t-9.96\t-9.92\t-9.84\t-9.57\t-9.39\t-9.26\t-7.76\t-6.49\t-7.34\t\n",
      "\n",
      "-9.96\t-9.95\t-9.93\t-9.82\t-9.53\t-9.59\t-9.38\t-8.84\t-8.31\t-8.88\t\n",
      "\n",
      "-9.95\t-9.91\t-9.89\t-9.86\t-9.89\t-9.70\t-9.54\t-9.55\t-9.16\t-9.43\t\n",
      "\n",
      "-9.98\t-9.95\t-9.92\t-9.53\t-9.68\t-9.79\t-9.79\t-9.69\t-9.47\t-9.52\t\n",
      "\n",
      "-9.97\t-9.96\t-9.92\t-10.00\t-9.67\t-9.82\t-9.79\t-9.81\t-9.74\t-9.68\t\n",
      "\n",
      "-9.95\t-9.96\t-9.89\t-9.99\t-9.90\t-9.90\t-9.96\t-9.87\t-9.77\t-9.89\t\n",
      "\n",
      "-9.97\t-9.99\t-10.00\t0.00\t-9.89\t-9.91\t-9.93\t-9.91\t-9.90\t-9.90\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 10 10\n",
      "starting episode 0\n",
      "starting episode 100\n",
      "starting episode 200\n",
      "starting episode 300\n",
      "starting episode 400\n",
      "\n",
      "\n",
      "-9.97\t-9.96\t-9.91\t-9.86\t-9.71\t-9.42\t-9.03\t-8.34\t-7.81\t-7.77\t\n",
      "\n",
      "-9.97\t-9.96\t-9.94\t-9.93\t-9.82\t-9.33\t-8.68\t-7.46\t-5.95\t-7.01\t\n",
      "\n",
      "-9.97\t-9.96\t-9.92\t-9.86\t-9.72\t-9.20\t-8.32\t-6.04\t0.00\t-5.09\t\n",
      "\n",
      "-9.98\t-9.96\t-9.93\t-9.94\t-9.64\t-9.41\t-8.67\t-7.63\t-6.06\t-6.29\t\n",
      "\n",
      "-9.98\t-9.96\t-9.97\t-9.87\t-9.49\t-9.49\t-9.12\t-8.72\t-8.32\t-7.79\t\n",
      "\n",
      "-9.96\t-9.91\t-9.86\t-10.00\t-9.77\t-9.78\t-9.57\t-9.49\t-9.33\t-9.09\t\n",
      "\n",
      "-9.98\t-9.94\t-9.88\t-9.93\t-9.69\t-9.75\t-9.73\t-9.63\t-9.51\t-9.52\t\n",
      "\n",
      "-9.99\t-9.95\t-9.98\t0.00\t-9.88\t-9.96\t-9.80\t-9.89\t-9.81\t-9.82\t\n",
      "\n",
      "-9.99\t-9.93\t-9.91\t0.00\t-9.79\t-9.93\t-9.83\t-9.83\t-9.89\t-9.82\t\n",
      "\n",
      "-9.99\t-9.88\t-9.77\t0.00\t-10.00\t-10.00\t-9.89\t-9.84\t-9.90\t-9.83\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_first_visit(10,500,0.9,wind=[0,0,1,2,1,0])\n",
    "MC_first_visit(10,500,0.9,wind=[0,0,3,3,3,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F칬rsta simuleringen (wind=[0,0,1,2,1,0]):\n",
    "GAMMA: 0.9: V칛rdefunktionen p친verkas av vind och gamma. Starkare vind leder till l칛gre v칛rden, s칛rskilt i omr친den d칛r vinden 칛r starkare. Systemet tar h칛nsyn till 칬kad sv친righet att navigera genom vinden.\n",
    "\n",
    "Andra simuleringen (wind=[0,0,3,3,3,0]):\n",
    "GAMMA: 0.9: P친verkan av vind och gamma 칛r liknande den f칬rsta simuleringen. Starkare vind ger l칛gre v칛rden, och systemet tar h칛nsyn till 칬kad sv친righet att r칬ra sig genom de omr친den d칛r vinden 칛r stark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be5ffb9",
   "metadata": {},
   "source": [
    "## Exploring Start Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "428bf8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_exploring_starts(X =6,Y=1,wind=[0, 0, 1, 2, 1, 0]):\n",
    "    grid = WindyGrid(X ,X, wind)\n",
    "    GAMMA = Y\n",
    "    print(\"GAMMA:\",Y)\n",
    "    print(\"Size:\",X,X)\n",
    "   \n",
    "    # Initialize Q, returns, and pairs visited\n",
    "    Q = {}          \n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "    \n",
    "    # initialize a random policy\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = np.random.choice(grid.possibleActions)\n",
    "    \n",
    "    for i in range(1000000):  \n",
    "        if i % 50000 == 0:\n",
    "            print('starting episode', i)\n",
    "        statesActionsReturns = []\n",
    "        observation = np.random.choice(grid.stateSpace)\n",
    "        action = np.random.choice(grid.possibleActions)\n",
    "        grid.setState(observation)\n",
    "        observation_, reward, done, info = grid.step(action)\n",
    "        memory = [(observation, action, reward)]\n",
    "        steps = 1\n",
    "        while not done:\n",
    "            action = policy[observation_]\n",
    "            steps += 1\n",
    "            observation, reward, done, info = grid.step(action)\n",
    "            if steps > 15 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation_, action, reward))\n",
    "            observation_ = observation\n",
    "\n",
    "        # append the terminal state\n",
    "        memory.append((observation_, action, reward))\n",
    "        \n",
    "        G = 0        \n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):\n",
    "            if last:\n",
    "                last = False  \n",
    "            else:\n",
    "                statesActionsReturns.append((state,action, G))\n",
    "            G = GAMMA*G + reward\n",
    "\n",
    "        statesActionsReturns.reverse()\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])                   \n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.argmax(values)\n",
    "                policy[state] = grid.possibleActions[best]\n",
    "            \n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9298a493",
   "metadata": {},
   "source": [
    "## Del 2\n",
    "\n",
    "\n",
    "- Anv칛nd  *exploring starts* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur 칛ndras policyn om man 칛ndra gamma till:\n",
    "    - 洧=0.5\n",
    "    - 洧=0,9\n",
    "    - 洧=0,95\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "        - Prova med 洧=0,9, vad h칛nder med policyn?\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42bdbc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.5\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-1.99913, -1.99917, -1.99914, -1.99808]\t[-1.9982, -1.99856, -1.9992, -1.99613]\t[-1.99651, -1.99627, -1.99841, -1.99222]\t[-1.99235, -1.99316, -1.99623, -1.98438]\t[-1.98448, -1.98508, -1.99262, -1.96875]\t[-1.96899, -1.9375, -1.9844, -1.96921]\t\n",
      "\n",
      "[-1.99912, -1.99909, -1.99908, -1.99826]\t[-1.99807, -1.99815, -1.99987, -1.99612]\t[-1.99625, -1.99638, -1.99814, -1.99219]\t[-1.99612, -1.99219, -1.99617, -1.99622]\t[-1.9844, -1.98445, -1.99221, -1.96878]\t[-1.96891, -1.875, -1.98441, -1.93759]\t\n",
      "\n",
      "[-1.99925, -1.99958, -1.99909, -1.99806]\t[-1.99817, -1.99909, -1.99917, -1.99611]\t[-1.99626, -1.99624, -1.99809, -1.9922]\t[-1.9922, -1.99222, -1.99612, -1.98438]\t[-1.98441, -1.98441, -1.99224, -1.96877]\t[-1.93757, -1.75, -1.9844, -1.87507]\t\n",
      "\n",
      "[-1.99919, -1.9991, -1.99963, -1.99903]\t[-1.99819, -1.99805, -1.99961, -1.99811]\t[-1.99611, -1.99617, -1.99816, -1.99617]\t[-1.99221, -1.9961, -1.99612, -1.98438]\t[-1.9844, -1.98439, -1.99611, -1.93753]\t[-1.87507, -1.5, -1.96877, -1.75003]\t\n",
      "\n",
      "[-1.99956, -1.9991, -1.99907, -1.99806]\t[-1.99911, -1.9981, -1.99908, -1.9961]\t[-1.99617, -1.99808, -1.99814, -1.99219]\t[-1.99221, -1.9922, -1.99611, -1.98439]\t[0, 0, 0, 0]\t[-1.75004, -1.75004, -1.0, -1.50007]\t\n",
      "\n",
      "[-1.99909, -1.9991, -1.99912, -1.99807]\t[-1.99807, -1.99809, -1.99909, -1.9961]\t[-1.99621, -1.99612, -1.99904, -1.99219]\t[-1.99611, -1.9922, -1.99614, -1.9844]\t[-1.98439, -1.0, -1.99221, -1.75004]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tD\tU\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tU\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-6.86559, -6.9545, -6.88067, -6.51575]\t[-6.54046, -6.57462, -6.87498, -6.13013]\t[-6.14206, -6.15158, -6.55681, -5.69825]\t[-5.91878, -5.72308, -6.13525, -5.21719]\t[-5.22133, -5.21992, -5.75819, -4.68566]\t[-4.69611, -4.09511, -5.22702, -4.69436]\t\n",
      "\n",
      "[-6.89114, -6.91691, -6.93978, -6.53037]\t[-6.53273, -6.55504, -6.9856, -6.15218]\t[-6.16804, -6.13857, -6.53659, -5.69758]\t[-6.15927, -5.69718, -6.14152, -6.13123]\t[-5.2319, -5.22255, -5.70828, -4.68618]\t[-4.68985, -3.43908, -5.22772, -4.09974]\t\n",
      "\n",
      "[-6.94789, -7.20473, -6.88829, -6.54231]\t[-6.61279, -6.88785, -6.90247, -6.14024]\t[-6.12864, -6.16999, -6.53348, -5.69669]\t[-5.70461, -5.70983, -6.13332, -5.21736]\t[-5.22925, -5.22665, -5.70433, -4.68594]\t[-4.09982, -2.71003, -5.22129, -3.44064]\t\n",
      "\n",
      "[-6.8885, -6.86932, -7.20936, -6.8811]\t[-6.58632, -6.51675, -7.18702, -6.54824]\t[-6.13054, -6.16606, -6.54307, -6.15296]\t[-5.70312, -6.13274, -6.14275, -5.21809]\t[-5.22517, -5.22268, -6.13074, -4.09731]\t[-3.44226, -1.90001, -4.68992, -2.71176]\t\n",
      "\n",
      "[-7.18805, -6.89986, -6.87279, -6.52421]\t[-6.89094, -6.52795, -6.87989, -6.13523]\t[-6.22407, -6.52504, -6.53957, -5.69592]\t[-5.70165, -5.70032, -6.15701, -5.21703]\t[0, 0, 0, 0]\t[-2.71172, -2.71165, -1.0, -1.9037]\t\n",
      "\n",
      "[-6.89158, -6.89835, -6.91048, -6.52034]\t[-6.5416, -6.54082, -6.91159, -6.12862]\t[-6.14595, -6.1401, -6.87763, -5.69936]\t[-6.13054, -5.71094, -6.14447, -5.21987]\t[-5.2198, -1.0, -5.71781, -2.71169]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "D\tD\tU\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tU\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.95\n",
      "Size: 6 6\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-8.68873, -8.65028, -8.69519, -8.03353]\t[-8.06377, -8.05862, -8.67423, -7.39765]\t[-7.47236, -7.53576, -8.10212, -6.73245]\t[-6.74532, -6.7455, -7.51956, -6.03389]\t[-6.08072, -6.04167, -6.76405, -5.29835]\t[-5.31844, -4.5246, -6.03886, -5.31892]\t\n",
      "\n",
      "[-8.7028, -8.65895, -8.65861, -8.02625]\t[-8.05673, -8.04906, -8.68665, -7.3953]\t[-7.40561, -7.41237, -8.05812, -6.73397]\t[-7.40256, -6.73329, -7.41053, -7.40529]\t[-6.06589, -6.04996, -6.75541, -5.29987]\t[-5.30983, -3.71, -6.05825, -4.5361]\t\n",
      "\n",
      "[-8.63755, -9.2214, -8.67154, -8.03848]\t[-8.06557, -8.64298, -8.66355, -7.39559]\t[-7.42054, -7.40277, -8.06059, -6.73201]\t[-6.73432, -6.74453, -7.41385, -6.03342]\t[-6.04136, -6.04427, -6.7369, -5.2993]\t[-4.54846, -2.85253, -6.04013, -3.71908]\t\n",
      "\n",
      "[-8.67721, -8.62549, -9.19757, -8.64526]\t[-8.03991, -8.03738, -9.20909, -8.0354]\t[-7.43015, -7.40037, -8.02981, -7.39573]\t[-6.73693, -7.40272, -7.41547, -6.03465]\t[-6.03883, -6.0383, -7.39752, -4.52585]\t[-3.72199, -1.95005, -5.30371, -2.85574]\t\n",
      "\n",
      "[-9.22102, -8.65968, -8.63377, -8.02637]\t[-8.63275, -8.06842, -8.64533, -7.39588]\t[-7.41308, -8.03229, -8.03999, -6.73216]\t[-6.73949, -6.73684, -7.40263, -6.03868]\t[0, 0, 0, 0]\t[-2.85569, -2.859, -1.0, -1.95334]\t\n",
      "\n",
      "[-8.65504, -8.65741, -8.64416, -8.04183]\t[-8.03767, -8.06815, -8.65119, -7.41124]\t[-7.39985, -7.4026, -8.64084, -6.73307]\t[-7.40755, -6.73423, -7.39765, -6.045]\t[-6.04299, -1.0, -6.73437, -2.8525]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "D\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tR\t\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts(6,0.5,wind=[0, 0, 2, 3, 2, 0])\n",
    "MC_exploring_starts(6,0.9,wind=[0, 0, 2, 3, 2, 0])\n",
    "MC_exploring_starts(6,0.95,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMMA: 0.5\n",
    "Slutv칛rdesfunktionen verkar minska gradvis fr친n det h칬gsta v칛rdet l칛ngst upp till v칛nster till det l칛gsta v칛rdet l칛ngst ner till h칬ger.\n",
    "\n",
    "GAMMA: 0.9\n",
    "Slutv칛rdesfunktionen verkar vara mindre ben칛gen att minska snabbt j칛mf칬rt med gamma 0.5. Det finns en 칬kad tendens till att h칬ga v칛rden sprider sig 칬ver omr친det.\n",
    "\n",
    "GAMMA: 0.95\n",
    "Slutv칛rdesfunktionen verkar ha en 칛nnu mindre ben칛genhet att minska snabbt. Det finns en 칬kad utj칛mning av h칬ga v칛rden 칬ver hela omr친det."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7f92dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 8 8\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-6.63444, -6.56437, -6.63036, -6.15865]\t[-6.15302, -6.23117, -6.73056, -5.70572]\t[-5.80481, -5.71858, -6.19972, -5.22225]\t[-5.35982, -5.26968, -5.73351, -4.68801]\t[-4.74432, -4.70543, -5.22317, -4.09857]\t[-4.1036, -3.44216, -4.69033, -4.72314]\t[-4.6955, -4.09546, -4.13332, -5.22421]\t[-5.22254, -4.70123, -4.68881, -5.22402]\t\n",
      "\n",
      "[-6.7038, -6.56719, -6.57393, -6.15349]\t[-6.17294, -6.18371, -6.61247, -5.69768]\t[-5.7287, -5.727, -6.19422, -5.21855]\t[-5.74815, -5.22531, -5.70804, -5.71454]\t[-4.69788, -4.77646, -5.22995, -4.09568]\t[-4.10183, -2.71349, -4.69937, -4.10216]\t[-4.6903, -3.43917, -3.45096, -4.69321]\t[-5.23107, -4.09801, -4.09569, -4.69646]\t\n",
      "\n",
      "[-6.55935, -6.88218, -6.57581, -6.18306]\t[-6.27097, -6.53254, -6.57003, -5.70226]\t[-5.72909, -5.7374, -6.14707, -5.21928]\t[-5.22707, -5.23584, -5.71958, -4.6886]\t[-4.70354, -4.81203, -5.23931, -4.09583]\t[-3.44651, -1.90004, -4.70102, -3.46419]\t[-4.09796, -2.71192, -2.71735, -4.09793]\t[-4.69081, -3.43962, -3.44209, -4.10308]\t\n",
      "\n",
      "[-6.61375, -6.51472, -6.87858, -6.53266]\t[-6.19163, -6.13061, -6.88955, -6.16695]\t[-5.70072, -5.72903, -6.1586, -5.75039]\t[-5.23352, -5.70579, -5.70741, -4.68802]\t[0, 0, 0, 0]\t[-2.71653, -2.71454, -1.0, -2.71329]\t[-3.4478, -3.44178, -1.90046, -3.44195]\t[-4.09782, -4.10085, -2.71034, -3.44219]\t\n",
      "\n",
      "[-6.87836, -6.54795, -6.52151, -6.12723]\t[-6.54474, -6.15069, -6.53932, -5.698]\t[-5.70519, -6.13918, -6.19307, -5.2196]\t[-5.2357, -5.23696, -5.71985, -4.68778]\t[-4.69802, -1.0, -5.2254, -2.71662]\t[-1.90339, -3.44465, -1.9007, -3.44194]\t[-2.71023, -4.09833, -2.71459, -4.10052]\t[-3.44192, -4.69088, -3.43936, -4.09803]\t\n",
      "\n",
      "[-6.52355, -6.55246, -6.5307, -6.14487]\t[-6.1427, -6.20548, -6.5371, -5.69705]\t[-5.71962, -5.70633, -6.53832, -5.2193]\t[-5.711, -5.22689, -5.72669, -4.70444]\t[-4.70739, -1.9034, -5.22103, -1.9]\t[-2.71511, -2.71035, -2.71317, -4.09796]\t[-3.4426, -3.44207, -3.439, -4.68855]\t[-4.09566, -4.10055, -4.09842, -4.69072]\t\n",
      "\n",
      "[-6.53244, -6.54562, -6.57028, -6.1343]\t[-6.1615, -6.19592, -6.5554, -5.70807]\t[-6.14352, -5.70637, -6.14555, -5.21968]\t[-5.22185, -5.23238, -6.13192, -1.0]\t[-1.0, -2.71315, -5.22807, -2.71385]\t[-3.44178, -3.44319, -1.90012, -3.44187]\t[-4.09804, -4.09827, -2.71018, -4.09793]\t[-4.68804, -4.69078, -3.439, -4.09785]\t\n",
      "\n",
      "[-6.5687, -6.5399, -6.53627, -6.14072]\t[-6.16276, -6.15316, -6.54088, -5.71261]\t[-5.70441, -5.72665, -6.14367, -5.23478]\t[-5.21891, -5.23148, -5.69953, -1.90325]\t[-1.9, -1.9065, -5.23231, -3.439]\t[-2.71083, -3.44282, -2.71343, -4.0951]\t[-3.4396, -4.10359, -3.44328, -4.68826]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\tD\tL\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\tD\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\tD\tD\t\n",
      "\n",
      "D\tD\tU\tR\t\tL\tL\tL\t\n",
      "\n",
      "R\tR\tR\tR\tD\tL\tU\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tD\tL\tU\t\n",
      "\n",
      "R\tR\tR\tR\tU\tL\tL\tL\t\n",
      "\n",
      "R\tR\tR\tR\tU\tU\tU\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts(8,0.9,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcd0efc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 10 10\n",
      "starting episode 0\n",
      "starting episode 50000\n",
      "starting episode 100000\n",
      "starting episode 150000\n",
      "starting episode 200000\n",
      "starting episode 250000\n",
      "starting episode 300000\n",
      "starting episode 350000\n",
      "starting episode 400000\n",
      "starting episode 450000\n",
      "starting episode 500000\n",
      "starting episode 550000\n",
      "starting episode 600000\n",
      "starting episode 650000\n",
      "starting episode 700000\n",
      "starting episode 750000\n",
      "starting episode 800000\n",
      "starting episode 850000\n",
      "starting episode 900000\n",
      "starting episode 950000\n",
      "[-7.19725, -6.98567, -7.25034, -6.67004]\t[-6.7863, -6.88453, -7.26878, -6.18669]\t[-6.40165, -6.62071, -7.15188, -5.71547]\t[-5.75464, -5.89609, -6.76459, -5.21744]\t[-5.47549, -5.23362, -5.76989, -4.69355]\t[-4.73632, -4.10172, -5.27879, -4.12004]\t[-4.10926, -3.45081, -4.72296, -3.44288]\t[-3.44419, -2.71055, -4.10499, -2.76407]\t[-2.72078, -1.90122, -3.47712, -3.46308]\t[-3.45787, -2.712, -2.72561, -3.44871]\t\n",
      "\n",
      "[-7.23241, -6.9199, -6.95729, -6.62303]\t[-6.76124, -6.68341, -6.97416, -6.13853]\t[-6.29119, -6.39148, -6.97002, -5.70333]\t[-6.20041, -5.70839, -6.15896, -6.15393]\t[-5.2261, -5.26754, -5.76655, -4.68581]\t[-4.71248, -3.44595, -5.23723, -3.45395]\t[-4.10519, -2.72566, -4.10914, -2.71322]\t[-3.44872, -1.90674, -3.44842, -1.90065]\t[-2.72478, -1.0, -2.71526, -2.71469]\t[-3.45696, -1.90591, -1.90059, -2.71484]\t\n",
      "\n",
      "[-6.96079, -7.6426, -6.92778, -6.53926]\t[-6.63282, -6.88203, -7.36863, -6.12711]\t[-6.36428, -6.23623, -6.87142, -5.69685]\t[-5.71828, -5.70685, -6.36348, -5.26302]\t[-5.22961, -5.24194, -5.7443, -4.68602]\t[-4.16323, -4.116, -5.24145, -2.71533]\t[-3.45385, -3.48559, -3.45, -1.90265]\t[-2.71595, -2.7201, -2.71484, -1.0]\t[0, 0, 0, 0]\t[-2.71557, -2.71493, -1.0, -1.90506]\t\n",
      "\n",
      "[-6.89114, -6.9293, -7.24146, -7.35177]\t[-6.56991, -6.74701, -7.20775, -6.51767]\t[-6.38075, -6.12867, -6.68444, -6.14866]\t[-5.70031, -6.13633, -6.37279, -5.22054]\t[-5.2377, -5.22872, -6.15066, -4.09904]\t[-3.46226, -4.69831, -4.71899, -3.44194]\t[-2.75591, -4.1143, -4.12382, -2.71032]\t[-1.9063, -3.45383, -3.45872, -1.9002]\t[-1.0, -2.71979, -2.7156, -2.71563]\t[-1.90563, -3.44472, -1.9, -2.71487]\t\n",
      "\n",
      "[-7.2592, -6.93736, -6.93874, -6.52445]\t[-6.88066, -6.57945, -6.97244, -6.19702]\t[-6.32616, -6.55967, -6.54885, -5.74818]\t[-5.72484, -5.73382, -6.23058, -5.2181]\t[-5.24057, -4.71132, -5.70768, -3.43925]\t[-4.11085, -5.25294, -4.10041, -4.09678]\t[-3.44167, -4.69007, -4.69825, -3.45298]\t[-2.71064, -4.09918, -4.09937, -2.71591]\t[-1.9002, -3.46108, -3.4456, -3.44788]\t[-2.71019, -4.10114, -2.71755, -3.44378]\t\n",
      "\n",
      "[-6.9323, -6.94067, -6.91043, -6.56494]\t[-6.54191, -6.57943, -6.9326, -6.17802]\t[-6.17048, -6.14758, -6.87013, -5.73385]\t[-6.14065, -5.74916, -6.19333, -5.2176]\t[-5.22936, -4.09636, -5.76228, -4.11288]\t[-4.70208, -5.69961, -4.68818, -4.72522]\t[-4.1082, -5.23309, -5.24961, -4.09596]\t[-3.43939, -4.69301, -4.6989, -3.45866]\t[-2.71065, -4.14628, -4.09921, -4.12673]\t[-3.43931, -4.69444, -3.45701, -4.12512]\t\n",
      "\n",
      "[-6.91237, -6.86561, -6.94905, -6.52878]\t[-6.58254, -6.56694, -6.95406, -6.13041]\t[-6.54048, -6.20765, -6.54188, -5.70164]\t[-5.70344, -5.73214, -6.53854, -4.68628]\t[-4.68643, -4.70367, -5.71623, -4.69118]\t[-5.24726, -5.22752, -5.23336, -5.21785]\t[-4.68656, -5.70003, -5.70726, -4.69095]\t[-4.09555, -5.23045, -5.22169, -4.12992]\t[-3.45378, -4.72426, -4.70769, -4.69383]\t[-4.09539, -5.22687, -4.2368, -4.69406]\t\n",
      "\n",
      "[-6.92797, -6.53418, -6.88566, -6.51671]\t[-6.58261, -6.12735, -6.87875, -6.15012]\t[-6.13268, -6.15983, -6.58849, -5.70163]\t[-5.7587, -5.69924, -6.14744, -4.09558]\t[-4.09705, -5.22438, -5.71047, -5.23311]\t[-5.70247, -5.72014, -4.68725, -5.7035]\t[-5.21915, -6.14093, -5.23104, -5.23284]\t[-4.68825, -5.71959, -5.70324, -4.70713]\t[-4.10026, -5.25534, -5.22651, -5.23003]\t[-4.69456, -5.70879, -4.70076, -5.23397]\t\n",
      "\n",
      "[-6.8745, -6.12938, -6.53869, -6.14743]\t[-6.52116, -5.69774, -6.53409, -5.71489]\t[-6.19059, -6.16708, -6.54859, -5.22369]\t[-5.71535, -5.21703, -6.18591, -4.68559]\t[-4.68766, -4.69828, -5.22815, -5.6994]\t[-5.23404, -5.70841, -5.22111, -6.12958]\t[-5.69749, -6.15191, -5.71365, -5.71255]\t[-5.22117, -6.14829, -6.12954, -5.30824]\t[-4.68792, -5.71338, -5.72356, -5.74515]\t[-5.22673, -6.13357, -5.24699, -5.70851]\t\n",
      "\n",
      "[-6.54268, -6.14322, -6.13014, -5.69659]\t[-6.15458, -5.70284, -6.12993, -5.21802]\t[-6.15165, -5.70283, -6.51665, -4.68642]\t[-5.70236, -4.68947, -6.14311, -5.21703]\t[-5.2241, -5.21703, -4.68649, -5.2291]\t[-5.71928, -5.70981, -5.22002, -6.15241]\t[-6.13159, -6.14859, -5.69929, -6.14881]\t[-5.72025, -6.16656, -6.18173, -5.70126]\t[-5.22858, -5.72054, -6.16082, -6.13559]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\tR\tD\tD\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\tR\tR\tD\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tR\tR\t\tL\t\n",
      "\n",
      "U\tR\tD\tR\tR\tR\tR\tR\tU\tL\t\n",
      "\n",
      "R\tR\tR\tR\tR\tR\tU\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tR\tD\tL\tR\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tR\tU\tR\tU\tU\tU\tU\t\n",
      "\n",
      "R\tD\tR\tR\tU\tL\tU\tU\tU\tU\t\n",
      "\n",
      "D\tD\tR\tR\tU\tL\tU\tU\tU\tU\t\n",
      "\n",
      "R\tR\tR\tD\tL\tL\tL\tR\tU\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_exploring_starts(10,0.9,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analys av 8x8:\n",
    "Observationer:\n",
    "Generellt sett verkar policyn anpassa sig till den givna vinden (wind) och f칬rs칬ker hitta en v칛g till m친let.\n",
    "I vissa omr친den d칛r vinden 칛r stark, kan policyn v칛lja att g친 ner친t (D) f칬r att dra nytta av vinden.\n",
    "Policyn verkar vara k칛nslig f칬r b친de hinder och vindens p친verkan, vilket kan leda till alternativa v칛gar f칬r att n친 m친let.\n",
    "\n",
    "Analys av 10x10:\n",
    "Observationer:\n",
    "M칬nstret 칛r liknande 8x8, men p친 grund av en st칬rre milj칬 har policyn mer utrymme att anpassa sig och hitta effektivare v칛gar.\n",
    "Policyns beteende verkar vara mer robust i st칬rre milj칬er och kan hantera de l친nga vindstr칛ckorna b칛ttre.\n",
    "\n",
    "Sammanfattning:\n",
    "Policyn tycks anpassa sig v칛l till b친de vind och hinder f칬r att n친 m친let.\n",
    "St칬rre milj칬er till친ter mer flexibilitet och b칛ttre anpassning till vindf칬rh친llandena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054b42a6",
   "metadata": {},
   "source": [
    "## On-policy first visit Monte Carlo for $\\varepsilon$-soft policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba2dcfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MC_without_exploring_starts(X =6,Y=0.9,wind=[0, 0, 1, 2, 1, 0]):\n",
    "    grid = WindyGrid(X,X, wind)\n",
    "    GAMMA = Y\n",
    "    EPS = 0.4\n",
    "\n",
    "    Q = {}\n",
    "    returns = {}\n",
    "    pairsVisited = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.actionSpace.keys():\n",
    "            Q[(state, action)] = 0\n",
    "            returns[(state,action)] = 0\n",
    "            pairsVisited[(state,action)] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = grid.possibleActions\n",
    "\n",
    "    for i in range(1000000):\n",
    "        statesActionsReturns = []\n",
    "        if i % 100000 == 0:\n",
    "            print('starting episode', i)\n",
    "        observation, done = grid.reset()       \n",
    "        memory = []\n",
    "        steps = 0\n",
    "        while not done:       \n",
    "            if len(policy[observation]) > 1:\n",
    "                action = np.random.choice(policy[observation])\n",
    "            else:\n",
    "                action = policy[observation]\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25 and not done:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "\n",
    "        #append the terminal state\n",
    "        memory.append((observation, action, reward))\n",
    "\n",
    "        G = 0        \n",
    "        last = True # start at t = T - 1\n",
    "        for state, action, reward in reversed(memory):                                    \n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                statesActionsReturns.append((state,action,G))           \n",
    "            G = GAMMA*G + reward\n",
    "        statesActionsReturns.reverse()\n",
    "\n",
    "        statesAndActions = []\n",
    "        for state, action, G in statesActionsReturns:\n",
    "            if (state, action) not in statesAndActions:\n",
    "                pairsVisited[(state,action)] += 1\n",
    "                returns[(state,action)] += (1 / pairsVisited[(state,action)])*(G-returns[(state,action)])                   \n",
    "                Q[(state,action)] = returns[(state,action)]\n",
    "                statesAndActions.append((state,action))\n",
    "                values = np.array([Q[(state,a)] for a in grid.possibleActions])\n",
    "                best = np.random.choice(np.where(values==values.max())[0])                    \n",
    "                rand = np.random.random()\n",
    "                if rand < 1 - EPS:\n",
    "                    policy[state] = grid.possibleActions[best]\n",
    "                else:                        \n",
    "                    policy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(policy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88293b06",
   "metadata": {},
   "source": [
    "## Del 3\n",
    "- Anv칛nd *without exploring starts* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur 칛ndras policyn om man 칛ndra gamma till:\n",
    "    - 洧=0.5\n",
    "    - 洧=0,9\n",
    "    - 洧=0,95\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "        - Prova med 洧=0,9, vad h칛nder med policyn?\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2dc828a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-2.0, -1.99997, -2.00001, -1.99999]\t[-2.00002, -1.99998, -2.00001, -2.00013]\t[-2.00014, -2.00009, -2.00013, -2.00061]\t[-2.0014, -2.00074, -2.00006, -1.99821]\t[-2.00012, -2.00024, -2.00012, -1.99478]\t[-2.00025, -1.98498, -2.00032, -2.00024]\t\n",
      "\n",
      "[-2.00001, -1.99998, -2.00001, -1.99992]\t[-2.00001, -1.99994, -2.00001, -1.99979]\t[-2.00002, -1.99999, -2.00002, -1.99937]\t[-2.00788, -2.00823, -2.00767, -2.00692]\t[-2.00193, -2.00208, -2.00226, -2.00178]\t[-2.00048, -1.95676, -2.00045, -2.0005]\t\n",
      "\n",
      "[-2.00002, -2.0, -2.00001, -1.99994]\t[-1.99999, -1.99999, -2.00002, -1.99978]\t[-2.00001, -1.99983, -2.00004, -1.99939]\t[-2.00034, -1.99987, -2.00031, -1.99834]\t[-2.00411, -2.0041, -2.01622, -2.00342]\t[-2.00094, -1.87588, -2.00099, -2.00091]\t\n",
      "\n",
      "[-2.00004, -2.00001, -2.00004, -2.0]\t[-2.00003, -1.99996, -2.00004, -1.99998]\t[-2.0003, -2.00002, -2.00028, -2.0002]\t[-2.0191, -2.02075, -2.01987, -2.01753]\t[-2.05642, -2.00645, -2.00367, -2.00394]\t[-2.00182, -1.64355, -2.00701, -2.00194]\t\n",
      "\n",
      "[-2.00011, -2.00273, -2.00138, -2.00001]\t[-2.00016, -2.00009, -2.00037, -1.99985]\t[-2.00002, -2.00016, -2.0002, -1.99946]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-2.00369, -1.87864, -1.0, -2.00356]\t\n",
      "\n",
      "[-2.00024, -2.00022, -2.00018, -2.04741]\t[-2.0003, -2.00019, -2.0002, -2.04029]\t[-2.00071, -2.71186, -2.0004, -2.00002]\t[0, 0, 0, 0]\t[-2.03627, -1.0, -2.01284, -2.01448]\t\n",
      "\n",
      "--------------------\n",
      "D\tU\tD\tR\tL\tD\t\n",
      "\n",
      "U\tR\tR\tR\tR\tL\t\n",
      "\n",
      "R\tR\tD\tL\tL\tD\t\n",
      "\n",
      "R\tD\tL\tR\tR\tU\t\n",
      "\n",
      "R\tR\tR\tUDLR\t\tU\t\n",
      "\n",
      "L\tD\tR\tUDLR\tU\tD\t\n",
      "\n",
      "--------------------\n",
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-13.14281, -12.87529, -13.14281, -12.8295]\t[-13.51562, -13.17736, -13.51608, -13.06289]\t[-13.97223, -13.96868, -13.96901, -13.25491]\t[-14.4774, -14.47633, -14.47458, -13.31429]\t[-14.97262, -14.97779, -14.97584, -13.13613]\t[-15.53335, -12.60689, -15.52739, -15.53097]\t\n",
      "\n",
      "[-13.56824, -13.3648, -13.55849, -13.17088]\t[-13.74426, -13.55437, -13.87939, -13.35867]\t[-14.3314, -13.87203, -14.17498, -13.57684]\t[-18.75329, -18.29681, -18.72101, -18.59613]\t[-17.41143, -17.43773, -17.43492, -17.40724]\t[-16.13801, -11.51242, -16.1371, -16.14548]\t\n",
      "\n",
      "[-14.12334, -13.87957, -14.26783, -13.73625]\t[-14.27988, -14.07565, -14.28834, -13.76139]\t[-14.46838, -14.34394, -14.85657, -14.13996]\t[-16.96601, -16.02349, -15.90886, -16.08427]\t[-17.91143, -17.80249, -17.84402, -17.82928]\t[-16.81923, -9.47536, -16.83413, -16.82074]\t\n",
      "\n",
      "[-14.73447, -14.44569, -14.59094, -14.21828]\t[-14.81419, -14.64282, -14.89359, -14.4647]\t[-14.91223, -14.99198, -15.28609, -15.20781]\t[-20.22597, -20.38369, -20.50112, -19.29785]\t[-18.45893, -18.44384, -18.39892, -18.3934]\t[-17.58843, -5.9327, -17.56697, -17.58759]\t\n",
      "\n",
      "[-15.33835, -15.37169, -15.2905, -14.91989]\t[-15.43806, -15.13735, -15.51673, -15.03311]\t[-15.89812, -16.3429, -17.10339, -15.49378]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-18.42634, -10.82495, -1.0, -18.42826]\t\n",
      "\n",
      "[-16.76768, -16.25694, -16.49436, -17.0609]\t[-16.29395, -16.02407, -16.1529, -15.48692]\t[-15.95614, -17.28919, -16.62659, -15.81218]\t[0, 0, 0, 0]\t[-20.33875, -1.0, -20.43221, -20.32611]\t\n",
      "\n",
      "--------------------\n",
      "R\tR\tR\tR\tR\tD\t\n",
      "\n",
      "R\tR\tL\tD\tR\tD\t\n",
      "\n",
      "R\tR\tR\tL\tD\tR\t\n",
      "\n",
      "R\tR\tU\tR\tR\tD\t\n",
      "\n",
      "R\tR\tR\tUDLR\t\tU\t\n",
      "\n",
      "D\tR\tR\tUDLR\tD\tR\t\n",
      "\n",
      "--------------------\n",
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-28.96407, -28.4444, -28.96406, -28.90735]\t[-29.49332, -29.49658, -29.51487, -29.1989]\t[-30.65838, -30.60905, -30.35924, -29.38077]\t[-31.54076, -31.53006, -31.28653, -28.43683]\t[-32.1209, -32.21364, -32.20519, -27.38671]\t[-32.8809, -25.61621, -32.84576, -32.85257]\t\n",
      "\n",
      "[-29.45969, -28.87746, -29.46749, -28.44808]\t[-29.66281, -28.76258, -29.9538, -29.18604]\t[-30.70961, -30.05208, -30.29886, -29.39945]\t[-32.46724, -30.38851, -32.507, -32.40369]\t[-34.38018, -34.42439, -34.5061, -34.40641]\t[-33.54002, -22.71175, -33.55655, -33.54616]\t\n",
      "\n",
      "[-30.23235, -29.9422, -30.13691, -29.53938]\t[-30.45078, -29.65891, -30.43513, -28.74981]\t[-30.01847, -29.54337, -30.26446, -28.9298]\t[-32.99038, -32.96236, -32.83629, -29.11639]\t[-35.31787, -35.33225, -35.20477, -35.26592]\t[-34.27585, -17.9577, -34.27807, -34.28298]\t\n",
      "\n",
      "[-30.91258, -30.84996, -30.97382, -30.44097]\t[-31.00553, -30.21534, -31.0269, -29.74776]\t[-30.91373, -29.85489, -31.58949, -30.16873]\t[-38.32567, -38.7037, -38.85966, -38.26556]\t[-35.7755, -35.72514, -35.78439, -35.67161]\t[-34.98592, -10.36885, -34.91336, -34.99997]\t\n",
      "\n",
      "[-31.63963, -31.63497, -31.65539, -31.2618]\t[-31.63957, -31.5232, -31.67033, -30.19929]\t[-30.42264, -30.96402, -32.31609, -31.11932]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-35.78169, -19.98813, -1.0, -35.78276]\t\n",
      "\n",
      "[-33.34327, -32.28273, -32.9331, -32.45121]\t[-32.37745, -32.2344, -32.56069, -30.63933]\t[-28.8272, -33.18298, -33.52967, -39.57885]\t[0, 0, 0, 0]\t[-37.55427, -1.0, -37.4614, -37.21013]\t\n",
      "\n",
      "--------------------\n",
      "U\tR\tU\tL\tR\tU\t\n",
      "\n",
      "R\tD\tU\tD\tU\tR\t\n",
      "\n",
      "R\tR\tL\tR\tL\tD\t\n",
      "\n",
      "R\tR\tR\tD\tR\tD\t\n",
      "\n",
      "R\tR\tD\tUDLR\t\tR\t\n",
      "\n",
      "D\tR\tD\tUDLR\tL\tR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_starts(6,0.5,wind=[0, 0, 2, 3, 2, 0])\n",
    "MC_without_exploring_starts(6,0.9,wind=[0, 0, 2, 3, 2, 0])\n",
    "MC_without_exploring_starts(6,0.95,wind=[0, 0, 2, 3, 2, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma = 0.5:\n",
    "The agent seems to prioritize short-term rewards. You can see that the values tend to decrease quickly as you move away from positive rewards, indicating a strong discount on future rewards.\n",
    "\n",
    "Gamma = 0.9:\n",
    "With a higher gamma, the agent considers a balance between short-term and long-term rewards. The values decrease gradually, indicating a smoother transition in discounting future rewards. The agent is likely to consider both immediate and future gains.\n",
    "\n",
    "Gamma = 0.95:\n",
    "An even higher gamma could lead to the agent focusing more on long-term rewards. The values might decrease more slowly, indicating a stronger consideration of future rewards. The agent might be more patient in its decision-making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a1a3d19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-2.0, -2.0, -2.0, -2.0]\t[-2.00003, -2.00003, -2.00003, -2.00004]\t[-2.00012, -2.00017, -2.00011, -2.02577]\t[-2.04985, -2.07643, -2.00577, -2.07419]\t[-2.00036, -2.00082, -2.34832, -2.00067]\t[-2.00283, -2.01371, -2.00173, -2.00106]\t[-2.00612, -2.00819, -2.00646, -2.00204]\t[-2.00632, -2.03818, -2.02268, -2.00311]\t\n",
      "\n",
      "[-2.0, -2.0, -2.0, -2.00001]\t[-2.00001, -2.00001, -2.00003, -2.0009]\t[-2.00007, -2.01656, -2.00006, -2.03414]\t[-2.00062, -2.00066, -2.00061, -2.00073]\t[-2.00226, 0, -2.0192, -2.57812]\t[-2.00192, -3.15632, -2.28906, -2.00258]\t[-2.00435, -2.0096, -2.00226, -2.07227]\t[-2.14453, -4.3125, -2.57812, -2.00468]\t\n",
      "\n",
      "[-2.00001, -2.00002, -2.00002, -2.00002]\t[-2.00002, -2.00002, -2.00002, -2.00003]\t[-2.00027, -2.00016, -2.00015, -2.00013]\t[-2.00052, -2.00074, -2.00067, -2.00065]\t[0, 0, -11.25, -2.07227]\t[0, 0, -6.625, -2.00028]\t[-2.00141, -11.25, 0, -2.03613]\t[-2.07227, 0, -6.625, 0]\t\n",
      "\n",
      "[-2.00003, -2.00004, -2.00004, -2.00008]\t[-2.00009, -2.00005, -2.00006, -2.00011]\t[-2.00035, -2.00039, -2.00072, -2.00031]\t[-2.00141, -2.00124, -2.00149, -2.00202]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-20.5, 0, 0, -26.0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[-2.00014, -2.0001, -2.0001, -2.00009]\t[-2.00092, -2.0003, -2.00013, -2.00011]\t[-2.04519, -2.0002, -2.00097, -2.00029]\t[-2.00113, 0, -2.00452, -2.00677]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[-2.00025, -2.22592, -2.00026, -2.00022]\t[-2.0025, -2.00037, -2.00042, -2.00026]\t[-2.00049, -2.00057, -2.00061, -2.00072]\t[-2.00226, -2.00226, 0, -2.03613]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[-4.8469, -2.00081, -3.42409, -2.00175]\t[-2.00091, -2.51439, -2.00055, -2.00075]\t[-2.00141, -2.00081, -2.00395, -2.0026]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "[-2.00175, -9.40141, -2.38702, -2.01519]\t[-6.625, -2.00141, -2.03004, -2.00113]\t[-2.00258, -2.00452, -2.00181, -2.00677]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t\n",
      "\n",
      "--------------------\n",
      "R\tU\tU\tL\tU\tR\tR\tR\t\n",
      "\n",
      "R\tL\tR\tL\tL\tU\tL\tD\t\n",
      "\n",
      "U\tU\tR\tD\tD\tL\tL\tD\t\n",
      "\n",
      "R\tD\tR\tL\t\tUDLR\tD\tUDLR\t\n",
      "\n",
      "R\tR\tD\tR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "R\tU\tU\tL\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "L\tL\tD\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "U\tR\tL\tUDLR\tUDLR\tUDLR\tUDLR\tUDLR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_starts(8,0.5,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0509f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting episode 0\n",
      "starting episode 100000\n",
      "starting episode 200000\n",
      "starting episode 300000\n",
      "starting episode 400000\n",
      "starting episode 500000\n",
      "starting episode 600000\n",
      "starting episode 700000\n",
      "starting episode 800000\n",
      "starting episode 900000\n",
      "[-2.00024, -1.99995, -2.0, -1.99987]\t[-2.00002, -1.99988, -2.00053, -1.99963]\t[-2.00006, -2.00002, -2.00014, -1.99895]\t[-2.00003, -2.00004, -2.00003, -1.99691]\t[-2.00006, -2.00007, -2.00006, -1.99113]\t[-2.00012, -1.98172, -2.00056, -1.97741]\t[-2.00022, -1.94274, -2.00038, -1.94834]\t[-2.0013, -1.90756, -2.00165, -1.87594]\t[-2.00261, -1.74551, -2.00232, -1.95422]\t[-2.00676, -1.8981, -2.0058, -2.00645]\t\n",
      "\n",
      "[-2.00001, -1.99997, -2.00001, -1.99986]\t[-1.99996, -1.99989, -2.00001, -1.99961]\t[-2.00002, -1.99967, -1.99998, -1.99897]\t[-2.01166, -2.01641, -2.01369, -2.01682]\t[-2.00161, -2.00136, -2.00128, -2.01337]\t[-2.00052, -1.9534, -2.00045, -1.96209]\t[-2.00033, -1.87325, -1.98521, -1.85677]\t[-1.96078, -1.65607, -2.00082, -1.64914]\t[-2.00067, -1.0, -2.00126, -1.87902]\t[-2.00529, -1.64847, -1.96482, -2.00897]\t\n",
      "\n",
      "[-2.00002, -2.00002, -2.00002, -1.99992]\t[-2.0, -2.00003, -2.00002, -1.9996]\t[-1.99976, -1.99968, -2.00018, -1.99893]\t[-2.02316, -2.03392, -2.08237, -2.03802]\t[-2.00645, -2.00256, -2.0026, -2.00194]\t[-2.00097, -1.98146, -2.00091, -1.87591]\t[-1.98121, -1.94914, -2.01039, -1.64909]\t[-1.91612, -1.89268, -2.01658, -1.0]\t[0, 0, 0, 0]\t[-2.0167, -1.90132, -1.0, -2.0132]\t\n",
      "\n",
      "[-2.0001, -2.0001, -2.00011, -2.00009]\t[-2.00007, -2.00007, -2.00006, -2.0001]\t[-2.00043, -2.00076, -2.00022, -2.00044]\t[-2.05232, -2.01702, -2.02499, -2.04791]\t[-2.03563, -2.01138, -2.01002, -2.21576]\t[-2.00258, -2.00198, -2.00387, -1.95198]\t[-1.95897, -1.98668, -2.00443, -1.87003]\t[-1.82915, -2.02979, -2.06848, -1.65523]\t[-1.0, -2.03062, -2.06212, -1.97345]\t[-1.83495, -2.12478, -1.81092, -2.07797]\t\n",
      "\n",
      "[-2.00017, -2.00031, -2.0003, -2.00023]\t[-2.00038, -2.00017, -2.00017, -2.00007]\t[-2.0006, -2.0002, -2.00052, -2.00048]\t[-2.0613, -2.14594, -2.22018, -2.02224]\t[-2.10934, -3.07443, -2.0148, -2.04658]\t[-2.01288, -2.01254, -2.00822, -1.99957]\t[-2.01793, -2.0332, -2.01401, -1.95529]\t[-2.13264, -2.32238, -2.01695, -1.89204]\t[-1.79201, -2.01847, -2.12948, -2.0156]\t[-1.99411, -3.08634, -2.03544, -2.07435]\t\n",
      "\n",
      "[-2.00073, -2.0009, -2.00057, -2.00085]\t[-2.0008, -2.00044, -2.00055, -2.0006]\t[-2.00187, -2.00384, -2.00252, -2.00241]\t[0, -2.00226, -2.00903, -2.03613]\t[-2.03648, -2.72266, -2.12646, -2.12556]\t[-2.0442, -2.2009, -2.03484, -2.17902]\t[-2.07465, -2.0471, -2.0942, -2.60423]\t[-2.04081, -2.04396, -2.0698, -3.20946]\t[-2.43222, -2.23486, -2.0712, -2.03137]\t[-2.22262, -4.72287, -2.54199, -6.67468]\t\n",
      "\n",
      "[-2.00102, -2.00132, -2.00282, -2.00132]\t[-2.00109, -2.00117, -2.00101, -2.00082]\t[-2.00294, -2.00154, -2.00254, -2.01313]\t[0, -1.99219, 0, 0]\t[-2.08431, -2.36133, -2.1084, -2.13369]\t[-2.99591, -2.05984, -2.05658, -2.16862]\t[-2.11864, -2.07339, -2.13851, -2.14004]\t[-2.17188, -3.99092, -2.14252, -2.25809]\t[-2.66846, -2.57451, -2.17034, -2.59017]\t[-4.3125, -2.07227, -2.14453, -2.14453]\t\n",
      "\n",
      "[-2.14453, -2.00258, -2.02597, -3.15625]\t[-2.0025, -2.00403, -3.17432, -2.0025]\t[-2.00452, -2.00282, -2.00113, -2.0079]\t[0, 0, 0, 0]\t[-20.5, -2.28906, 0, -2.01807]\t[-2.28906, -11.32227, -2.00903, -6.64307]\t[-2.09756, -2.30713, -8.9375, -2.07227]\t[-3.27368, -2.2168, -4.3125, -2.1084]\t[-3.54855, -2.2168, -2.65039, -2.28906]\t[-2.14453, -2.57812, -2.57812, -2.14453]\t\n",
      "\n",
      "[-2.0192, -2.00452, -2.00903, -2.00565]\t[-2.01807, -2.01129, 0, -1.99805]\t[0, 0, 0, -1.99609]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-39.0, 0, -26.0, -2.28906]\t[-6.625, 0, -2.57812, -2.61426]\t[-2.14453, -2.28906, -4.3125, 0]\t[0, -2.57812, 0, -2.36133]\t[-26.0, -2.28906, 0, -15.10417]\t\n",
      "\n",
      "[-2.00903, -2.07227, -20.5, -4.3125]\t[0, -2.00903, -6.625, -2.03613]\t[-2.07227, 0, 0, 0]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-39.0, -11.25, 0, 0]\t[0, 0, -6.625, 0]\t[0, -2.57812, -4.3125, -3.15625]\t[0, -4.3125, -3.15625, -11.25]\t\n",
      "\n",
      "--------------------\n",
      "D\tR\tR\tR\tR\tR\tD\tR\tL\tL\t\n",
      "\n",
      "R\tU\tU\tU\tL\tD\tR\tR\tD\tD\t\n",
      "\n",
      "R\tR\tR\tU\tR\tR\tL\tR\t\tL\t\n",
      "\n",
      "R\tL\tR\tD\tL\tR\tR\tU\tU\tD\t\n",
      "\n",
      "D\tU\tD\tR\tU\tD\tR\tR\tU\tU\t\n",
      "\n",
      "L\tL\tU\tU\tL\tL\tD\tU\tR\tR\t\n",
      "\n",
      "U\tR\tD\tU\tU\tR\tR\tR\tL\tU\t\n",
      "\n",
      "L\tD\tU\tUDLR\tL\tL\tR\tD\tD\tR\t\n",
      "\n",
      "U\tL\tD\tUDLR\tUDLR\tU\tD\tR\tL\tR\t\n",
      "\n",
      "U\tD\tL\tUDLR\tUDLR\tR\tD\tU\tU\tL\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_without_exploring_starts(10,0.5,wind=[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policyn verkar st칬ta p친 sv친righeter i att anpassa sig till den givna milj칬n och vindf칬rh친llandena.\n",
    "Det finns omr친den d칛r policyn inte n친r m친let och fastnar, som indikeras av 친terkommande v칛rden som -2.0.\n",
    "Policyn har sv친rt att hantera vissa vindstr칛ckor och hinder, vilket resulterar i suboptimala r칬relser och f칬rseningar i att n친 m친let.\n",
    "I vissa fall verkar det som policyn g친r in i en loop och kan inte hitta en effektiv v칛g till m친let."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a2a0d",
   "metadata": {},
   "source": [
    "## Off-Policy Monte Carlo prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc7f57b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MC_off_policy_prediction(X =6,Y=0.9, wind=[0, 0, 1, 2, 1, 0]):\n",
    "    grid = WindyGrid(X,X, wind)\n",
    "    GAMMA = Y\n",
    "\n",
    "    print(\"GAMMA:\",Y)\n",
    "    print(\"Size:\",X,X)\n",
    "\n",
    "    Q = {}\n",
    "    C = {}\n",
    "    for state in grid.stateSpacePlus:\n",
    "        for action in grid.possibleActions:\n",
    "            Q[(state,action)] = 0\n",
    "            C[(state,action)] = 0\n",
    "    \n",
    "    targetPolicy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        targetPolicy[state] = np.random.choice(grid.possibleActions)\n",
    "\n",
    "    for i in range(1000000):\n",
    "        if i % 100000 == 0:\n",
    "            print(i)            \n",
    "        behaviorPolicy = {}\n",
    "        for state in grid.stateSpace:\n",
    "            behaviorPolicy[state] = grid.possibleActions\n",
    "        memory = []\n",
    "        observation, done = grid.reset()\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            action = np.random.choice(behaviorPolicy[observation])\n",
    "            observation_, reward, done, info = grid.step(action)\n",
    "            steps += 1\n",
    "            if steps > 25:\n",
    "                done = True\n",
    "                reward = -steps\n",
    "            memory.append((observation, action, reward))\n",
    "            observation = observation_\n",
    "        memory.append((observation, action, reward))\n",
    "        \n",
    "        G = 0\n",
    "        W = 1\n",
    "        last = True\n",
    "        for (state, action, reward) in reversed(memory):            \n",
    "            if last:\n",
    "                last = False\n",
    "            else:\n",
    "                C[state,action] += W\n",
    "                Q[state,action] += (W / C[state,action])*(G-Q[state,action])\n",
    "                prob = 1 if action in targetPolicy[state] else 0\n",
    "                W *= prob/(1/len(behaviorPolicy[state]))\n",
    "                if W == 0:\n",
    "                    break\n",
    "            G = GAMMA*G + reward\n",
    "    printQ(Q, grid)\n",
    "    printPolicy(targetPolicy,grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27b9cc6",
   "metadata": {},
   "source": [
    "## Del 4\n",
    "- Anv칛nd *off-policy prediction* Monte Carlo Metoden\n",
    "\n",
    "1. 칐ka vindstyrkan med en enhet.\n",
    "    - Hur 칛ndras slutv칛rdesfunktionen?\n",
    "\n",
    "\n",
    "2. Hur 칛ndras policyn om man 칛ndra gamma till:\n",
    "    - 洧=0.5\n",
    "    - 洧=0,9\n",
    "    - 洧=0,95\n",
    "\n",
    "\n",
    "3. Testa rutn칛tsv칛rlden i storlekarna:\n",
    "    - 8x8\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?\n",
    "        - Prova med 洧=0,9, vad h칛nder med policyn?\n",
    "    - 10x10\n",
    "        - 츿ndra p친 vinden, vad h칛nder med policyn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "406a1b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.5\n",
      "Size: 6 6\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-10.71822, -15.65889, -12.34482, -3.62715]\t[-11.02789, -7.01795, -7.40019, -7.11491]\t[-12.20539, -14.11802, -12.91215, -11.94801]\t[-11.9366, -9.70344, -15.26897, -14.59943]\t[-14.39834, -7.76413, -12.54181, -12.6446]\t[-15.77219, -11.73522, -14.5374, -15.70233]\t\n",
      "\n",
      "[-13.15019, -5.65206, -14.53301, -12.79667]\t[-15.18637, -13.53124, -9.60344, -15.36218]\t[-7.31196, -17.92787, -14.49805, -11.17038]\t[-18.27904, -19.07607, -17.27642, -18.4919]\t[-21.43665, -9.10704, -15.30995, -20.73424]\t[-14.65261, -15.89583, -17.9774, -13.44308]\t\n",
      "\n",
      "[-8.4978, -14.73219, -8.24958, -12.48932]\t[-12.9796, -15.3868, -14.24971, -16.14793]\t[-19.20996, -16.24813, -18.07917, -16.54291]\t[-3.10576, -15.19053, -25.44942, -15.61807]\t[-16.67507, -20.85293, -16.39573, -19.52158]\t[-18.25447, -18.6963, -16.54243, -18.76527]\t\n",
      "\n",
      "[-14.89202, -14.80185, -15.83627, -12.45649]\t[-12.36147, -15.67557, -11.51872, -7.8353]\t[-13.77151, -12.65753, -9.77406, -13.71386]\t[-20.76248, -9.68003, -20.05314, -15.4783]\t[-22.21071, -21.51466, -23.72983, -23.49901]\t[-20.02088, -8.62289, -19.99059, -15.66076]\t\n",
      "\n",
      "[-6.47405, -15.08549, -15.90142, -16.89276]\t[-13.52703, -17.40137, -17.57887, -10.72894]\t[-24.32442, -17.99045, -12.95343, -12.87619]\t[-10.80632, -20.11091, -5.95484, -5.93311]\t[0, 0, 0, 0]\t[-21.09552, -2.6734, -4.27979, -9.56288]\t\n",
      "\n",
      "[-10.25383, -5.49356, -16.46934, -10.28539]\t[-11.62451, -7.37337, -12.0535, -19.93302]\t[-19.1651, -22.95652, -22.73779, -5.57317]\t[0, 0, 0, 0]\t[-29.71429, -31.20388, -31.65385, -13.60804]\t\n",
      "\n",
      "--------------------\n",
      "U\tR\tU\tU\tR\tR\t\n",
      "\n",
      "U\tL\tD\tR\tL\tR\t\n",
      "\n",
      "L\tL\tL\tD\tL\tD\t\n",
      "\n",
      "R\tD\tL\tR\tL\tU\t\n",
      "\n",
      "L\tD\tL\tL\t\tL\t\n",
      "\n",
      "R\tU\tD\tL\tD\tD\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.9\n",
      "Size: 6 6\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-37.15788, -37.03036, -38.15072, -37.19023]\t[-37.74344, -36.72033, -35.8771, -38.40274]\t[-35.73789, -32.33439, -32.50711, -36.74015]\t[-37.55807, -37.77437, -35.15326, -36.77314]\t[-38.74149, -38.01385, -38.16889, -35.20363]\t[-37.48436, -37.46998, -34.78296, -38.11615]\t\n",
      "\n",
      "[-38.65227, -37.34344, -37.54547, -34.51491]\t[-36.06084, -37.14732, -37.77877, -38.36403]\t[-36.15132, -35.86407, -38.68, -31.97613]\t[-39.31589, -39.32671, -37.72065, -37.61294]\t[-39.35284, -39.84969, -34.28437, -37.83909]\t[-38.82213, -38.48282, -39.77885, -38.7954]\t\n",
      "\n",
      "[-39.15325, -39.31046, -38.81505, -39.52777]\t[-37.3313, -39.00331, -39.25294, -36.64958]\t[-38.19454, -38.0593, -39.32301, -38.36529]\t[-40.45743, -40.66067, -36.50078, -40.75353]\t[-38.48553, -38.65677, -36.43813, -39.3787]\t[-37.90607, -30.857, -37.12606, -39.33254]\t\n",
      "\n",
      "[-39.60016, -38.1467, -39.28436, -38.76827]\t[-39.18822, -39.77872, -38.82566, -38.22948]\t[-37.88244, -37.98478, -37.62111, -39.77236]\t[-39.02373, -40.08736, -40.17112, -39.07387]\t[-39.59698, -35.35148, -40.31939, -39.92894]\t[-40.09322, -38.16184, -32.9634, -40.22606]\t\n",
      "\n",
      "[-36.06531, -38.10436, -35.89774, -38.86853]\t[-38.60795, -35.65541, -31.02511, -38.51699]\t[-40.229, -38.73936, -39.58523, -39.34725]\t[-39.69827, -40.4834, -39.7139, -40.82739]\t[0, 0, 0, 0]\t[-37.71815, -39.48277, -4.71094, -39.51426]\t\n",
      "\n",
      "[-38.29002, -39.62975, -39.5621, -39.72192]\t[-34.56512, -38.44277, -38.41115, -37.97575]\t[-32.94558, -39.52443, -39.18105, -40.79015]\t[0, 0, 0, 0]\t[-38.23494, -40.19209, -41.97129, -39.12181]\t\n",
      "\n",
      "--------------------\n",
      "L\tL\tU\tD\tD\tU\t\n",
      "\n",
      "U\tD\tL\tU\tD\tR\t\n",
      "\n",
      "D\tD\tR\tR\tU\tR\t\n",
      "\n",
      "L\tD\tL\tL\tD\tL\t\n",
      "\n",
      "U\tR\tR\tD\t\tD\t\n",
      "\n",
      "R\tL\tD\tR\tR\tU\t\n",
      "\n",
      "--------------------\n",
      "GAMMA: 0.95\n",
      "Size: 6 6\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-43.20933, -43.80559, -44.10524, -44.01389]\t[-44.14469, -43.35674, -43.91993, -43.78963]\t[-44.06974, -43.89746, -43.40487, -43.80944]\t[-43.0383, -43.88695, -43.72708, -43.70197]\t[-42.83239, -43.29091, -43.63308, -43.76875]\t[-43.7111, -43.89423, -43.5486, -43.85442]\t\n",
      "\n",
      "[-43.10277, -42.60908, -43.94315, -43.72714]\t[-44.19117, -43.8796, -43.09253, -44.23945]\t[-44.3035, -44.08792, -42.44738, -44.01631]\t[-43.94331, -44.46337, -44.29163, -43.71325]\t[-44.05672, -43.9225, -42.9745, -43.82999]\t[-43.96776, -43.6293, -43.59948, -43.80727]\t\n",
      "\n",
      "[-44.21228, -44.33969, -43.606, -44.40121]\t[-44.24847, -44.27365, -44.25277, -43.63672]\t[-44.0831, -44.40457, -44.27499, -44.30128]\t[-43.28242, -44.52675, -44.71025, -44.60287]\t[-43.17469, -43.51548, -43.84161, -43.6954]\t[-43.74125, -44.11039, -43.83592, -43.61616]\t\n",
      "\n",
      "[-44.43217, -43.76415, -44.18404, -44.40448]\t[-44.2478, -44.21958, -44.34642, -44.24743]\t[-44.14665, -44.45084, -44.55499, -43.79157]\t[-42.79882, -44.50396, -42.79516, -44.46135]\t[-43.28873, -42.76911, -42.73743, -42.84692]\t[-43.3701, -42.54059, -43.36737, -43.43488]\t\n",
      "\n",
      "[-44.36715, -44.16359, -44.11133, -44.21314]\t[-44.08936, -44.44299, -44.05906, -44.11969]\t[-43.57369, -44.17432, -44.2862, -44.17047]\t[-44.47593, -44.7595, -44.5051, -43.15262]\t[0, 0, 0, 0]\t[-43.3261, -43.50777, -5.03335, -44.72804]\t\n",
      "\n",
      "[-44.17362, -44.20016, -44.241, -43.83466]\t[-44.4377, -44.22375, -44.0186, -44.20753]\t[-44.22724, -44.25237, -44.31306, -44.14485]\t[0, 0, 0, 0]\t[-43.04434, -38.62, -39.99822, -36.64]\t\n",
      "\n",
      "--------------------\n",
      "D\tL\tU\tR\tL\tR\t\n",
      "\n",
      "L\tD\tD\tD\tD\tU\t\n",
      "\n",
      "L\tU\tL\tU\tR\tU\t\n",
      "\n",
      "U\tL\tU\tR\tL\tR\t\n",
      "\n",
      "D\tD\tR\tD\t\tU\t\n",
      "\n",
      "D\tL\tD\tL\tR\tL\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_prediction(6,0.5, [0, 0, 1, 2, 1, 0])\n",
    "MC_off_policy_prediction(6,0.9, [0, 0, 1, 2, 1, 0])\n",
    "MC_off_policy_prediction(6,0.95, [0, 0, 1, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAMMA = 0.5:\n",
    "Den l친ga diskonteringsfaktorn g칬r att agenten ger mindre vikt 친t framtida bel칬ningar.\n",
    "Det kan resultera i en kortare \"synvinkel\" d칛r endast n칛rliggande bel칬ningar 칛r betydelsefulla.\n",
    "Det kan ses i vissa fall d칛r agenten kanske inte verkar optimera f칬r det mest l친ngsiktiga f칬rdelaktiga beslutet.\n",
    "\n",
    "GAMMA = 0.9:\n",
    "En h칬gre diskonteringsfaktor ger mer vikt 친t framtida bel칬ningar.\n",
    "Agenten tar mer h칛nsyn till l친ngsiktiga konsekvenser av sina beslut.\n",
    "Detta kan resultera i en f칬rsiktighet och en str칛van att optimera f칬r ett mer l친ngsiktigt m친l.\n",
    "\n",
    "GAMMA = 0.95:\n",
    "En 칛nnu h칬gre diskonteringsfaktor g칬r att agenten blir 칛nnu mer inriktad p친 l친ngsiktiga bel칬ningar.\n",
    "Det kan g칬ra agenten mer ben칛gen att offra kortsiktiga bel칬ningar f칬r att uppn친 st칬rre l친ngsiktiga vinster.\n",
    "Observera att i detta fall kan det leda till mindre h칛nsyn till kortsiktiga vinster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "560a06c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 8 8\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-33.38676, -34.70979, -37.93479, -31.01806]\t[-33.01396, -36.79171, -35.38746, -38.99121]\t[-35.96386, -32.38546, -35.88426, -38.42559]\t[-36.49026, -35.08748, -35.52896, -30.45335]\t[-38.2124, -31.04574, -33.12158, -37.13681]\t[-39.34743, -29.41528, -38.5736, -38.71746]\t[-36.2378, -38.88799, -38.526, -25.76161]\t[-39.84433, -38.42086, -39.60174, -28.26196]\t\n",
      "\n",
      "[-30.40114, -37.6033, -38.40985, -36.65975]\t[-38.98797, -37.92558, -37.17248, -34.04191]\t[-38.9199, -38.12026, -27.61864, -35.10073]\t[-36.34852, -40.0459, -39.75427, -39.16561]\t[-39.67262, -37.21322, -39.27908, -39.03474]\t[-37.96333, -35.23268, -39.7147, -31.36915]\t[-39.19903, -35.15434, -38.34476, -35.22413]\t[-38.03693, -38.09592, -38.09634, -39.2727]\t\n",
      "\n",
      "[-39.01427, -37.44892, -38.19888, -37.54004]\t[-38.12041, -39.35991, -35.44625, -38.87136]\t[-39.75524, -40.27696, -35.35065, -39.67725]\t[-36.30404, -40.36309, -36.39933, -32.03213]\t[-39.77338, -40.25436, -39.73176, -39.697]\t[-39.64121, -39.55712, -40.04847, -37.09312]\t[-37.2238, -38.92911, -40.05604, -39.1829]\t[-39.27566, -40.17554, -39.39569, -39.20961]\t\n",
      "\n",
      "[-36.74748, -36.8459, -36.63671, -38.10567]\t[-36.19158, -35.57784, -35.63022, -40.04332]\t[-36.73949, -40.74126, -34.08917, -34.56854]\t[-41.32475, -38.52229, -29.34051, -40.60328]\t[0, 0, 0, 0]\t[-39.78599, -39.96458, -4.36604, -40.03797]\t[-39.39813, -40.26648, -39.93769, -39.64034]\t[-38.79804, -39.92505, -38.8128, -38.90911]\t\n",
      "\n",
      "[-38.56096, -37.9438, -38.358, -39.68543]\t[-39.88774, -38.0846, -38.13917, -39.72407]\t[-36.73792, -40.0196, -39.4785, -40.57495]\t[-40.23827, -40.01436, -40.34893, -39.2208]\t[-39.66576, -40.62769, -41.23162, -39.07097]\t[-39.74145, -40.033, -40.05711, -39.68347]\t[-40.01205, -40.25936, -40.53526, -40.34846]\t[-40.74795, -38.71048, -37.77198, -38.05229]\t\n",
      "\n",
      "[-37.23755, -39.65582, -39.95911, -36.50861]\t[-38.89558, -39.98875, -38.80339, -39.09552]\t[-40.66017, -40.81707, -36.0092, -38.94823]\t[-33.75665, -40.43259, -40.61196, -3.28741]\t[-8.67857, -37.42933, -35.85263, -40.4]\t[-39.32302, -39.98345, -39.77643, -38.47308]\t[-39.10752, -39.50973, -39.2369, -38.85819]\t[-39.63623, -39.85696, -40.48634, -38.58545]\t\n",
      "\n",
      "[-40.41812, -37.44726, -39.13806, -39.92684]\t[-40.30388, -40.16007, -38.9585, -38.46643]\t[-39.82895, -40.49225, -40.66763, -41.06697]\t[-38.52202, -41.43769, -36.36971, -39.68523]\t[-36.02857, -37.7, -42.1984, -26.0]\t[-39.29285, -39.22667, -35.11015, -38.34955]\t[-40.86954, -40.83277, -29.9, -40.06467]\t[-40.35724, -40.00384, -38.18359, -39.07807]\t\n",
      "\n",
      "[-33.14995, -39.65461, -39.90614, -36.91765]\t[-38.95324, -39.39313, -38.80166, -39.17905]\t[-39.44172, -38.23137, -38.03859, -34.2373]\t[0, 0, 0, 0]\t[-37.7, -26.0, -41.6, -41.6]\t[-33.2, -35.68276, -37.33091, -38.464]\t[-38.0, -31.85, -41.38803, -40.9568]\t\n",
      "\n",
      "--------------------\n",
      "D\tU\tU\tU\tL\tR\tD\tR\t\n",
      "\n",
      "D\tD\tR\tR\tU\tR\tR\tD\t\n",
      "\n",
      "L\tU\tR\tU\tR\tR\tU\tL\t\n",
      "\n",
      "R\tR\tD\tR\t\tD\tL\tD\t\n",
      "\n",
      "U\tL\tR\tD\tR\tD\tU\tU\t\n",
      "\n",
      "D\tU\tU\tL\tD\tL\tL\tD\t\n",
      "\n",
      "D\tU\tU\tL\tD\tD\tR\tD\t\n",
      "\n",
      "D\tU\tU\tR\tR\tR\tU\tR\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_prediction(8,0.9, [0, 0, 1, 2, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "455c795f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAMMA: 0.9\n",
      "Size: 10 10\n",
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "[-31.321, -35.43355, -36.72997, -34.89671]\t[-34.44834, -37.89669, -36.25853, -31.58773]\t[-35.4929, -36.11777, -35.93204, -32.45446]\t[-31.48135, -36.2289, -37.1795, -37.85073]\t[-31.26421, -35.20207, -27.14261, -35.46457]\t[-37.2303, -37.1701, -36.95786, -38.49325]\t[-39.38013, -32.04814, -37.79362, -34.67127]\t[-37.03926, -39.62635, -39.59036, -38.9121]\t[-40.22593, -38.93862, -40.18552, -39.87263]\t[-39.8266, -38.35926, -40.54031, -39.82219]\t\n",
      "\n",
      "[-36.98188, -33.97512, -34.23349, -39.19558]\t[-34.00614, -39.43199, -38.01651, -37.75582]\t[-36.28924, -38.0781, -38.53278, -40.48671]\t[-33.78653, -37.05266, -39.38818, -37.2783]\t[-40.00042, -37.03202, -40.08101, -33.87233]\t[-35.59036, -38.42471, -39.30617, -38.44632]\t[-38.11898, -34.90753, -38.41528, -38.08177]\t[-39.34439, -10.43822, -39.39738, -39.44095]\t[-38.83327, -4.54081, -39.42862, -39.18213]\t[-40.09143, -40.15345, -39.901, -40.78873]\t\n",
      "\n",
      "[-38.95805, -35.3539, -32.11662, -39.08226]\t[-38.84038, -40.18547, -36.86178, -36.60343]\t[-37.98356, -38.35571, -38.56103, -38.03405]\t[-40.36627, -40.93247, -40.74494, -34.9302]\t[-40.72119, -40.02728, -40.07461, -40.0316]\t[-40.13611, -39.60614, -39.24611, -39.29195]\t[-37.10816, -39.90325, -39.03364, -10.09105]\t[-40.20109, -40.05038, -39.9552, -4.28684]\t[0, 0, 0, 0]\t[-39.14592, -39.96331, -5.83393, -37.27024]\t\n",
      "\n",
      "[-38.97173, -37.92921, -39.11755, -37.92643]\t[-39.63858, -39.56456, -37.37202, -39.7573]\t[-39.34363, -40.3885, -39.53921, -36.69929]\t[-39.48104, -40.66254, -38.54682, -37.32613]\t[-40.34443, -40.23163, -39.60666, -39.4071]\t[-38.39589, -40.09561, -40.3946, -39.76831]\t[-39.01687, -39.84391, -40.46812, -39.92055]\t[-11.16602, -39.25829, -39.45385, -12.74841]\t[-5.86212, -24.94195, -40.34065, -33.68047]\t[-36.85538, -34.87898, -13.79149, -40.22466]\t\n",
      "\n",
      "[-38.61262, -39.84594, -39.82355, -38.53853]\t[-37.07592, -40.08089, -40.06802, -39.20531]\t[-39.40257, -39.39695, -39.2129, -40.34622]\t[-40.8173, -40.6698, -40.12424, -39.13943]\t[-41.31543, -39.66523, -41.14704, -40.96672]\t[-40.02505, -39.09031, -39.65181, -40.27373]\t[-39.66836, -39.55857, -39.11157, -39.41829]\t[-37.53998, -38.39059, -39.95581, -20.13072]\t[-18.99558, -39.0942, -39.34559, -26.26389]\t[-39.93242, -39.20902, -28.08669, -32.73637]\t\n",
      "\n",
      "[-39.8786, -39.59895, -39.81644, -40.13143]\t[-40.36187, -39.51344, -40.2463, -40.57256]\t[-40.18362, -38.74947, -39.06258, -40.26848]\t[-39.1412, -39.66914, -40.85953, -39.19153]\t[-42.34623, -36.8, -42.99325, -40.50933]\t[-39.3959, -40.34865, -40.2899, -40.17784]\t[-39.28376, -37.02868, -39.07207, -39.10901]\t[-40.88469, -41.2828, -41.60455, -38.58769]\t[-35.52519, -40.024, -41.29318, -39.90096]\t[-17.01257, -38.29521, -40.81147, -41.747]\t\n",
      "\n",
      "[-37.11118, -40.41698, -39.64346, -38.58262]\t[-40.70532, -40.03959, -39.51837, -40.49075]\t[-40.33114, -37.99318, -40.7105, -40.63379]\t[-41.89927, -37.232, -41.58, -39.12]\t[-41.6, -44.36718, -41.6, -26.0]\t[-38.59915, -40.42627, -38.72563, -39.61455]\t[-39.86667, -33.488, -39.18769, -39.8162]\t[-36.57429, -38.59915, -40.422, -36.55015]\t[-37.232, -41.872, -38.64865, -38.31579]\t[-32.93333, -36.4, -41.6, -37.7]\t\n",
      "\n",
      "[-38.93712, -40.26607, -36.17518, -38.76767]\t[-40.82978, -35.38607, -41.10219, -39.82043]\t[-40.19593, -41.96429, -39.57916, -37.84671]\t[-32.24, -43.01818, -42.51765, -26.0]\t[0, -49.4, -26.0, -44.72]\t[-36.4, -33.8, -37.12, -38.48]\t[-37.01176, -38.91034, -32.24, -43.41647]\t[-37.7, -38.76364, -43.144, -39.12]\t[-40.46286, -40.99852, -37.01176, -26.0]\t[-43.792, -36.4, -26.0, -26.0]\t\n",
      "\n",
      "[-40.13993, -31.88319, -40.22013, -38.50276]\t[-39.76191, -37.65613, -37.85306, -40.09259]\t[-41.4938, -41.1355, -40.57714, -39.62019]\t[0, 0, 0, 0]\t[-26.0, -26.0, 0, -26.0]\t[-26.0, -41.6, -41.6, -26.0]\t[-36.4, -26.0, -41.6, -36.4]\t[-39.37143, -26.0, -42.87333, -41.6]\t[-37.7, -26.0, -44.72, -41.6]\t[-26.0, -44.72, 0, 0]\t\n",
      "\n",
      "[-37.85075, -35.50013, -38.35995, -39.38449]\t[-39.7166, -40.24148, -39.97481, -40.93111]\t[-39.24055, -40.47111, -39.32738, -38.40779]\t[0, 0, 0, 0]\t[0, 0, 0, 0]\t[-26.0, -26.0, -26.0, 0]\t[0, -26.0, -49.4, -26.0]\t[-49.4, -26.0, -42.38737, -26.0]\t[-26.0, -49.4, -43.792, 0]\t\n",
      "\n",
      "--------------------\n",
      "D\tL\tL\tU\tR\tL\tU\tU\tU\tL\t\n",
      "\n",
      "U\tD\tL\tD\tL\tD\tD\tL\tL\tU\t\n",
      "\n",
      "D\tD\tU\tD\tD\tL\tU\tR\t\tU\t\n",
      "\n",
      "R\tD\tD\tL\tD\tL\tR\tD\tU\tU\t\n",
      "\n",
      "L\tD\tR\tR\tR\tR\tU\tD\tU\tL\t\n",
      "\n",
      "U\tR\tR\tD\tL\tL\tD\tR\tR\tR\t\n",
      "\n",
      "D\tD\tU\tU\tL\tR\tD\tD\tL\tD\t\n",
      "\n",
      "R\tR\tD\tL\tD\tD\tL\tU\tL\tR\t\n",
      "\n",
      "L\tL\tU\tD\tU\tR\tU\tD\tR\tU\t\n",
      "\n",
      "D\tU\tR\tR\tU\tL\tL\tU\tU\tU\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "MC_off_policy_prediction(10,0.9 ,[0, 0, 2, 3, 2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F칬r 8x8 milj칬n med gamma = 0.9:\n",
    "Slutv칛rdesfunktionen 칛r representerad som en matris av bel칬ningar i varje tillst친nd.\n",
    "Policyn representeras som en sekvens av handlingar f칬r varje tillst친nd.\n",
    "\n",
    "-Slutv칛rdesfunktionen:\n",
    "H칬gsta v칛rden ses n칛ra det nedre h칬gra h칬rnet, vilket indikerar att dessa omr친den har h칬ga f칬rv칛ntade avkastningar.\n",
    "\n",
    "-Policy:\n",
    "Handlingarna varierar beroende p친 det f칬rv칛ntade v칛rdet i varje tillst친nd.\n",
    "Agenten v칛ljer att r칬ra sig mot omr친den med h칬gre f칬rv칛ntade avkastningar.\n",
    "\n",
    "\n",
    "F칬r 10x10 milj칬n med gamma = 0.9:\n",
    "Resultaten f칬r칛ndras beroende p친 milj칬ns storlek och gamma-v칛rdet.\n",
    "\n",
    "-Slutv칛rdesfunktionen:\n",
    "H칛r kan vi se f칬rv칛ntade avkastningar 칬ver hela milj칬n. H칬ga v칛rden indikerar omr친den med h칬g f칬rv칛ntad avkastning.\n",
    "\n",
    "-Policy:\n",
    "Policyn varierar beroende p친 det f칬rv칛ntade v칛rdet i varje tillst친nd.\n",
    "Agenten r칬r sig mot omr친den med h칬ga f칬rv칛ntade avkastningar och undviker omr친den med l친ga f칬rv칛ntade avkastningar.\n",
    "\n",
    "Sammanfattning:\n",
    "칐kande gamma kan g칬ra att agenten fokuserar mer p친 l친ngsiktiga bel칬ningar.\n",
    "칐kande milj칬ns storlek p친verkar hur agenten utforskar och utnyttjar omr친den med olika f칬rv칛ntade avkastningar.\n",
    "Resultaten ger insikt i hur agentens beteende anpassar sig till olika scenarier och diskonteringsfaktorer.\n",
    "F칬r att ytterligare anpassa agentens beteende kan man experimentera med olika v칛rden p친 gamma och andra relevanta parametrar i Q-learning-algoritmen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
