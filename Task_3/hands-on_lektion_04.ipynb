{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc3471e",
   "metadata": {},
   "source": [
    "## GridWorld\n",
    "Ph.D Leonarod A, Espinosa, M.Sc Andrej Scherbakov-Parland, BIT Kristoffer Kuvaja Adolfsson\n",
    "\n",
    "### Bibliography:\n",
    "\n",
    "* Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\n",
    "http://incompleteideas.net/book/bookdraft2017nov5.pdf  (chapter 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d331c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dab2812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld(object):\n",
    "    \"\"\" Gridworld defined by m x n matrix with\n",
    "    terminal states at top left corner and bottom right corner.\n",
    "    State transitions are deterministic; attempting to move\n",
    "    off the grid leaves the state unchanged, and rewards are -1 on\n",
    "    each step. \n",
    "\n",
    "    In this implementation we model the environment like a game\n",
    "    where an agent moves around.\n",
    "    \"\"\"\n",
    "    def __init__(self, m, n):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.grid = np.zeros((m,n))\n",
    "        self.stateSpace = [i+1 for i in range(self.m*self.n-2)]\n",
    "        self.stateSpacePlus = [i for i in range(self.m*self.n)]        \n",
    "        self.actionSpace = {'up': -self.m, 'down': self.m, \n",
    "                            'left': -1, 'right': 1}\n",
    "        self.p = self.initP() # probability functions\n",
    "        self.agentPosition = np.random.choice(self.stateSpace)\n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 1\n",
    "\n",
    "    def getAgentRowAndColumn(self):\n",
    "        x = self.agentPosition // self.m\n",
    "        y = self.agentPosition % self.n\n",
    "        return x, y\n",
    "\n",
    "    def setState(self, state):\n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 0            \n",
    "        self.agentPosition = state        \n",
    "        x, y = self.getAgentRowAndColumn() \n",
    "        self.grid[x][y] = 1  \n",
    "\n",
    "    def offGridMove(self, newState, oldState):\n",
    "        # if we move into a row not in the grid\n",
    "        if newState not in self.stateSpacePlus:\n",
    "            return True\n",
    "        # if we're trying to wrap around to next row\n",
    "        elif oldState % self.m == 0 and newState  % self.m == self.m - 1:\n",
    "            return True\n",
    "        elif oldState % self.m == self.m - 1 and newState % self.m == 0:\n",
    "            return True\n",
    "        else:\n",
    "            return False        \n",
    "    def step(self, action):        \n",
    "        resultingState = self.agentPosition + self.actionSpace[action]\n",
    "        if not self.offGridMove(resultingState, self.agentPosition):\n",
    "            self.setState(resultingState)\n",
    "            return resultingState, -1, self.isTerminalState(resultingState), None\n",
    "        else:\n",
    "            return self.agentPosition, -1, self.isTerminalState(self.agentPosition), None\n",
    "    \n",
    "    def isTerminalState(self, state):\n",
    "        return state in self.stateSpacePlus and state not in self.stateSpace      \n",
    "\n",
    "    def initP(self):\n",
    "        \"\"\" construct state transition probabilities for\n",
    "        use in value function. P(s', r|s, a) is a dictionary\n",
    "        with keys corresponding to the functional arguments.\n",
    "        values are either 1 or 0.\n",
    "        Translations that take agent off grid leave the state unchanged.\n",
    "        (s', r|s, a)\n",
    "        (1, -1|1, 'up') = 1\n",
    "        (1, -1|2, 'left') = 1\n",
    "        (1, -1|3, 'left') = 0        \n",
    "        \"\"\"\n",
    "        P = {}\n",
    "        for state in self.stateSpace:\n",
    "            for action in self.actionSpace:\n",
    "                resultingState = state + self.actionSpace[action]\n",
    "                key = (state, -1, state, action) if self.offGridMove(resultingState, state) \\\n",
    "                                                 else (resultingState, -1, state, action)\n",
    "                P[key] = 1\n",
    "        return P\n",
    "\n",
    "    def render(self):\n",
    "        print('------------------------------------------')\n",
    "        for row in self.grid:\n",
    "            for col in row:\n",
    "                if col == 0:\n",
    "                    print('-', end='\\t')\n",
    "                elif col == 1:\n",
    "                    print('X', end='\\t')\n",
    "            print('\\n')\n",
    "        print('------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bbc5f",
   "metadata": {},
   "source": [
    "## Del 1 - Utvärdera policyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ca77fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatePolicy(grid, V, policy, GAMMA, THETA):\n",
    "    # policy evaluation for the random choice in gridworld\n",
    "    converged = False\n",
    "    iterations = 0 # <---\n",
    "    while not converged:\n",
    "        DELTA = 0\n",
    "        for state in grid.stateSpace:\n",
    "            oldV = V[state]\n",
    "            total = 0\n",
    "            weight = 1 / len(policy[state])           \n",
    "            for action in policy[state]:\n",
    "                grid.setState(state)\n",
    "                newState, reward, _, _ = grid.step(action)\n",
    "                key = (newState, reward, state, action)\n",
    "                total += weight*grid.p[key]*(reward+GAMMA*V[newState])\n",
    "            V[state] = total\n",
    "            DELTA = max(DELTA, np.abs(oldV-V[state]))\n",
    "            converged = True if DELTA < THETA else False\n",
    "        iterations = iterations + 1 # <---\n",
    "    print('iterations: ', iterations) # <----\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e5f8377",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printV(V, grid):\n",
    "    for idx, row in enumerate(grid.grid):\n",
    "        for idy, _ in enumerate(row):            \n",
    "            state = grid.m * idx + idy \n",
    "            print('%.2f' % V[state], end='\\t')\n",
    "        print('\\n')\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6057c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy(m, n, x, y):\n",
    "    grid = GridWorld(m,n) # Rutnät m x n\n",
    "    THETA = x # Konvergensvillkor\n",
    "    GAMMA = y\n",
    "    \n",
    "    # initialize V(s)\n",
    "    V = {}\n",
    "    for state in grid.stateSpacePlus:        \n",
    "        V[state] = 0\n",
    "    \n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = [key for key in grid.actionSpace.keys()]\n",
    "\n",
    "    \n",
    "    V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n",
    "    printV(V, grid)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8dac63",
   "metadata": {},
   "source": [
    "### Hands-on del 1\n",
    "- Implementera utvärderingspolicyfunktionen (evaluatePolicy) i en python-notebook.\n",
    "- Testa koden för 4x4, 8x8 och 10x10 rutnät.\n",
    "    - Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "    - Vad händer med tiden?\n",
    "- Vad händer om gamma är lika med 0,5, 0,9 för 4x4 och 8x8? Summera och  gör slutsatser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5b4f4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  4\n",
      "0.00\t-3.07\t-4.09\t-4.39\t\n",
      "\n",
      "-3.07\t-4.12\t-4.63\t-4.68\t\n",
      "\n",
      "-4.09\t-4.63\t-4.62\t-3.86\t\n",
      "\n",
      "-4.39\t-4.68\t-3.86\t0.00\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "get_policy(4,4,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c2d1be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  5\n",
      "0.00\t-3.54\t-4.77\t-5.21\t-5.36\t-5.42\t-5.44\t-5.44\t\n",
      "\n",
      "-3.54\t-4.71\t-5.38\t-5.69\t-5.82\t-5.87\t-5.89\t-5.90\t\n",
      "\n",
      "-4.77\t-5.38\t-5.77\t-5.97\t-6.07\t-6.11\t-6.12\t-6.13\t\n",
      "\n",
      "-5.21\t-5.69\t-5.97\t-6.12\t-6.19\t-6.22\t-6.24\t-6.24\t\n",
      "\n",
      "-5.36\t-5.82\t-6.07\t-6.19\t-6.25\t-6.28\t-6.27\t-6.23\t\n",
      "\n",
      "-5.42\t-5.87\t-6.11\t-6.22\t-6.28\t-6.27\t-6.15\t-5.90\t\n",
      "\n",
      "-5.44\t-5.89\t-6.12\t-6.24\t-6.27\t-6.15\t-5.63\t-4.53\t\n",
      "\n",
      "-5.44\t-5.90\t-6.13\t-6.24\t-6.23\t-5.90\t-4.53\t0.00\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "get_policy(8,8,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ab057bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  5\n",
      "0.00\t-3.54\t-4.77\t-5.21\t-5.36\t-5.42\t-5.44\t-5.44\t-5.44\t-5.45\t\n",
      "\n",
      "-3.54\t-4.71\t-5.38\t-5.69\t-5.82\t-5.87\t-5.89\t-5.90\t-5.90\t-5.90\t\n",
      "\n",
      "-4.77\t-5.38\t-5.77\t-5.97\t-6.07\t-6.11\t-6.12\t-6.13\t-6.13\t-6.13\t\n",
      "\n",
      "-5.21\t-5.69\t-5.97\t-6.12\t-6.19\t-6.22\t-6.24\t-6.24\t-6.24\t-6.25\t\n",
      "\n",
      "-5.36\t-5.82\t-6.07\t-6.19\t-6.25\t-6.28\t-6.29\t-6.29\t-6.29\t-6.30\t\n",
      "\n",
      "-5.42\t-5.87\t-6.11\t-6.22\t-6.28\t-6.30\t-6.31\t-6.32\t-6.32\t-6.31\t\n",
      "\n",
      "-5.44\t-5.89\t-6.12\t-6.24\t-6.29\t-6.31\t-6.32\t-6.32\t-6.31\t-6.26\t\n",
      "\n",
      "-5.44\t-5.90\t-6.13\t-6.24\t-6.29\t-6.32\t-6.32\t-6.30\t-6.17\t-5.92\t\n",
      "\n",
      "-5.44\t-5.90\t-6.13\t-6.24\t-6.29\t-6.32\t-6.31\t-6.17\t-5.64\t-4.53\t\n",
      "\n",
      "-5.45\t-5.90\t-6.13\t-6.25\t-6.30\t-6.31\t-6.26\t-5.92\t-4.53\t0.00\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "get_policy(10,10,0.9,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Den output du har gett ser ut att vara en utskrift av en värdefunktionsuppskattning för varje tillstånd i gridvärlden efter ett visst antal iterationer. Varje värde representerar den förväntade ackumulerade belöningen från det aktuella tillståndet, där negativa värden indikerar att det är bättre att undvika det tillståndet.\n",
    "\n",
    "- tiden har samma 0 sekunder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2\n",
      "0.00\t-1.42\t-1.62\t-1.65\t-1.66\t-1.66\t-1.66\t-1.66\t\n",
      "\n",
      "-1.42\t-1.68\t-1.74\t-1.75\t-1.76\t-1.76\t-1.76\t-1.76\t\n",
      "\n",
      "-1.62\t-1.74\t-1.77\t-1.77\t-1.77\t-1.77\t-1.77\t-1.77\t\n",
      "\n",
      "-1.65\t-1.75\t-1.77\t-1.78\t-1.78\t-1.78\t-1.78\t-1.78\t\n",
      "\n",
      "-1.66\t-1.76\t-1.77\t-1.78\t-1.78\t-1.78\t-1.78\t-1.78\t\n",
      "\n",
      "-1.66\t-1.76\t-1.77\t-1.78\t-1.78\t-1.78\t-1.78\t-1.78\t\n",
      "\n",
      "-1.66\t-1.76\t-1.77\t-1.78\t-1.78\t-1.78\t-1.78\t-1.61\t\n",
      "\n",
      "-1.66\t-1.76\t-1.77\t-1.78\t-1.78\t-1.78\t-1.61\t0.00\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "get_policy(8,8,0.9,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9821e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  2\n",
      "0.00\t-1.42\t-1.62\t-1.65\t\n",
      "\n",
      "-1.42\t-1.68\t-1.74\t-1.75\t\n",
      "\n",
      "-1.62\t-1.74\t-1.77\t-1.61\t\n",
      "\n",
      "-1.65\t-1.75\t-1.61\t0.00\t\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "get_policy(4,4,0.9,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det verkar som att förändringen av gamma-värdet har en påtaglig effekt på resultaten. Låt oss summera och dra några slutsatser baserat på de två olika gamma-värdena för 4x4-miljön:\n",
    "\n",
    "För gamma = 0.9:\n",
    "\n",
    "    Antal iterationer: 4\n",
    "    Värdefunktionen i varje tillstånd är ganska negativ, vilket indikerar att algoritmen värderar att undvika dessa tillstånd.\n",
    "    De negativa värdena är mer uttalade, och det finns en tydlig indikation på att undvika vissa tillstånd.\n",
    "\n",
    "För gamma = 0.5:\n",
    "\n",
    "    Antal iterationer: 2\n",
    "    Värdefunktionen är mindre negativ jämfört med gamma = 0.9. De negativa värdena är mindre uttalade.\n",
    "    Algoritmen verkar vara mindre \"straffande\" och ger mindre negativa värden för tillstånden.\n",
    "\n",
    "Slutsatser:\n",
    "\n",
    "    Påverkan av Gamma: Gamma styr vikten av framtida belöningar i algoritmen. En lägre gamma (0.5) gör att agensen i högre grad fokuserar på kortsiktiga belöningar, medan en högre gamma (0.9) ger större vikt åt långsiktiga belöningar.\n",
    "\n",
    "    Effekten på Värdefunktionen: För högt gamma (0.9) straffar algoritmen tydligt vissa tillstånd, vilket indikerar att den värderar att undvika dem mer. För lågt gamma (0.5) är straffet mindre uttalat, vilket gör att algoritmen är mindre benägen att undvika vissa tillstånd.\n",
    "\n",
    "    Konvergenstid: Förändringen av gamma påverkar även antalet iterationer som krävs för konvergens. En lägre gamma leder till snabbare konvergens (2 iterationer) jämfört med en högre gamma (4 iterationer).\n",
    "\n",
    "Valet av gamma är en viktig aspekt i förstärkningsinlärning och påverkar hur mycket agensen tar hänsyn till framtida belöningar. Det är ofta en avvägning mellan omedelbara och långsiktiga belöningar beroende på det specifika problemet och önskad inlärningsbeteende."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Och dra några slutsatser baserat på de två olika gamma-värdena för 8x8-miljön:\n",
    "För gamma = 0.9:\n",
    "\n",
    "    Antal iterationer: 5\n",
    "    Värdefunktionen är generellt sett mer negativ jämfört med gamma = 0.5. Algoritmen tenderar att straffa tillstånd hårdare.\n",
    "    De negativa värdena ökar gradvis mot övre vänstra och nedre högra hörnen, vilket indikerar att agensen undviker dessa områden.\n",
    "\n",
    "För gamma = 0.5:\n",
    "\n",
    "    Antal iterationer: 2\n",
    "    Värdefunktionen är mindre negativ jämfört med gamma = 0.9. De negativa värdena är mindre uttalade.\n",
    "    Algoritmen verkar vara mindre \"straffande\" och ger mindre negativa värden för tillstånden.\n",
    "\n",
    "Slutsatser:\n",
    "\n",
    "    Påverkan av Gamma: Som tidigare diskuterat styr gamma vikten av framtida belöningar. I detta fall har ett lägre gamma (0.5) resulterat i mindre negativa värden och snabbare konvergens jämfört med ett högre gamma (0.9).\n",
    "\n",
    "    Effekten på Värdefunktionen: För högt gamma (0.9) straffar algoritmen tydligt tillstånden hårdare, medan ett lägre gamma (0.5) ger mindre negativa värden.\n",
    "\n",
    "    Konvergenstid: Förändringen av gamma påverkar också antalet iterationer som krävs för konvergens. Ett lägre gamma leder till snabbare konvergens (2 iterationer) jämfört med ett högre gamma (5 iterationer).\n",
    "\n",
    "Sammanfattningsvis påverkar valet av gamma hur mycket agensen tar hänsyn till framtida belöningar och hur snabbt algoritmen konvergerar till en stabil uppskattning av den optimala värdefunktionen. Valet av gamma är en viktig faktor och beror på egenskaperna hos den specifika förstärkta inlärningsuppgiften och önskat beteende hos agensen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00454e56",
   "metadata": {},
   "source": [
    "## Del 2 - Förbättra policyn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c1378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvePolicy(grid, V, policy, GAMMA):\n",
    "    stable = True\n",
    "    newPolicy = {}\n",
    "    for state in grid.stateSpace:       \n",
    "        oldActions = policy[state]                \n",
    "        value = []\n",
    "        newAction = []\n",
    "        for action in policy[state]:\n",
    "            grid.setState(state)\n",
    "            weight = 1 / len(policy[state])\n",
    "            newState, reward, _, _ = grid.step(action)\n",
    "            key = (newState, reward, state, action)\n",
    "            value.append(np.round(weight*grid.p[key]*(reward+GAMMA*V[newState]), 2))\n",
    "            newAction.append(action)\n",
    "        value = np.array(value)        \n",
    "        best = np.where(value == value.max())[0]        \n",
    "        bestActions = [newAction[item] for item in best] \n",
    "        newPolicy[state] = bestActions\n",
    "\n",
    "        if oldActions != bestActions:\n",
    "            stable = False\n",
    "        \n",
    "    return stable, newPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6adc0a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_policy_iteration(m, n, x, y):\n",
    "\n",
    "    grid = GridWorld(m,n)\n",
    "    THETA = x\n",
    "    GAMMA = y\n",
    "    \n",
    "    # initialize V(s)\n",
    "\n",
    "    V = {}\n",
    "    for state in grid.stateSpacePlus:        \n",
    "        V[state] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = [key for key in grid.actionSpace.keys()]\n",
    "\n",
    "    # main loop for policy improvement\n",
    "    stable = False\n",
    "    while not stable:\n",
    "        V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n",
    "        stable, policy = improvePolicy(grid, V, policy, GAMMA)       \n",
    "        V = evaluatePolicy(grid, V, policy, GAMMA, THETA)\n",
    "\n",
    "        printV(V, grid) \n",
    "        time.sleep(2)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    printV(V,grid)\n",
    "\n",
    "    for state in policy:\n",
    "        print(state,policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6408bc",
   "metadata": {},
   "source": [
    "### Hands-on del 2.\n",
    "\n",
    "- Implementera den förbättrade policyfunktionen (improvePolicy).\n",
    "- Testa koden för 4x4, 8x8 och 10x10 rutnät.\n",
    "    - Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "    - Vad händer med tiden?\n",
    "- Hur olika är policyerna?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3baff7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-1.90\t\n",
      "\n",
      "-1.90\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "1 ['left']\n",
      "2 ['left']\n",
      "3 ['left']\n",
      "4 ['up']\n",
      "5 ['up', 'left']\n",
      "6 ['up']\n",
      "7 ['down']\n",
      "8 ['up']\n",
      "9 ['left']\n",
      "10 ['down', 'right']\n",
      "11 ['down']\n",
      "12 ['up']\n",
      "13 ['right']\n",
      "14 ['right']\n"
     ]
    }
   ],
   "source": [
    "get_policy_iteration(4,4,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f83f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t\n",
      "\n",
      "-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t\n",
      "\n",
      "-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-6.51\t\n",
      "\n",
      "-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-3.44\t-2.71\t\n",
      "\n",
      "-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-3.44\t-2.71\t-1.90\t\n",
      "\n",
      "-4.69\t-5.22\t-5.70\t-6.13\t-3.44\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-5.22\t-5.70\t-6.13\t-6.51\t-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "1 ['left']\n",
      "2 ['left']\n",
      "3 ['left']\n",
      "4 ['left']\n",
      "5 ['left']\n",
      "6 ['left']\n",
      "7 ['left']\n",
      "8 ['up']\n",
      "9 ['up', 'left']\n",
      "10 ['left']\n",
      "11 ['up']\n",
      "12 ['up']\n",
      "13 ['up']\n",
      "14 ['up']\n",
      "15 ['up']\n",
      "16 ['up']\n",
      "17 ['up']\n",
      "18 ['up', 'left']\n",
      "19 ['up']\n",
      "20 ['up']\n",
      "21 ['up']\n",
      "22 ['up']\n",
      "23 ['up']\n",
      "24 ['up']\n",
      "25 ['left']\n",
      "26 ['left']\n",
      "27 ['up', 'left']\n",
      "28 ['up']\n",
      "29 ['up']\n",
      "30 ['up']\n",
      "31 ['up']\n",
      "32 ['up']\n",
      "33 ['left']\n",
      "34 ['left']\n",
      "35 ['left']\n",
      "36 ['up', 'left']\n",
      "37 ['up']\n",
      "38 ['down']\n",
      "39 ['down']\n",
      "40 ['up']\n",
      "41 ['left']\n",
      "42 ['left']\n",
      "43 ['left']\n",
      "44 ['left']\n",
      "45 ['down', 'right']\n",
      "46 ['down']\n",
      "47 ['down']\n",
      "48 ['up']\n",
      "49 ['left']\n",
      "50 ['left']\n",
      "51 ['left']\n",
      "52 ['right']\n",
      "53 ['right']\n",
      "54 ['down', 'right']\n",
      "55 ['down']\n",
      "56 ['up']\n",
      "57 ['left']\n",
      "58 ['left']\n",
      "59 ['left']\n",
      "60 ['right']\n",
      "61 ['right']\n",
      "62 ['right']\n"
     ]
    }
   ],
   "source": [
    "get_policy_iteration(8,8,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a7534fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-6.51\t\n",
      "\n",
      "-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-6.51\t-6.86\t\n",
      "\n",
      "-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-6.51\t-6.86\t-7.18\t\n",
      "\n",
      "-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-6.51\t-6.86\t-7.18\t-7.46\t\n",
      "\n",
      "-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-6.51\t-6.86\t-4.69\t-4.10\t-3.44\t\n",
      "\n",
      "-4.69\t-5.22\t-5.70\t-6.13\t-6.51\t-6.86\t-4.69\t-4.10\t-3.44\t-2.71\t\n",
      "\n",
      "-5.22\t-5.70\t-6.13\t-6.51\t-6.86\t-4.69\t-4.10\t-3.44\t-2.71\t-1.90\t\n",
      "\n",
      "-5.70\t-6.13\t-6.51\t-6.86\t-7.18\t-4.10\t-3.44\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-6.13\t-6.51\t-6.86\t-7.18\t-7.46\t-3.44\t-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "1 ['left']\n",
      "2 ['left']\n",
      "3 ['left']\n",
      "4 ['left']\n",
      "5 ['left']\n",
      "6 ['left']\n",
      "7 ['left']\n",
      "8 ['left']\n",
      "9 ['left']\n",
      "10 ['up']\n",
      "11 ['up', 'left']\n",
      "12 ['left']\n",
      "13 ['up']\n",
      "14 ['up']\n",
      "15 ['up']\n",
      "16 ['up']\n",
      "17 ['up']\n",
      "18 ['up']\n",
      "19 ['up']\n",
      "20 ['up']\n",
      "21 ['up']\n",
      "22 ['up', 'left']\n",
      "23 ['up']\n",
      "24 ['up']\n",
      "25 ['up']\n",
      "26 ['up']\n",
      "27 ['up']\n",
      "28 ['up']\n",
      "29 ['up']\n",
      "30 ['up']\n",
      "31 ['left']\n",
      "32 ['left']\n",
      "33 ['up', 'left']\n",
      "34 ['up']\n",
      "35 ['up']\n",
      "36 ['up']\n",
      "37 ['up']\n",
      "38 ['up']\n",
      "39 ['up']\n",
      "40 ['up']\n",
      "41 ['left']\n",
      "42 ['left']\n",
      "43 ['left']\n",
      "44 ['up', 'left']\n",
      "45 ['up']\n",
      "46 ['up']\n",
      "47 ['up']\n",
      "48 ['up']\n",
      "49 ['up']\n",
      "50 ['up']\n",
      "51 ['left']\n",
      "52 ['left']\n",
      "53 ['left']\n",
      "54 ['left']\n",
      "55 ['up', 'left']\n",
      "56 ['up']\n",
      "57 ['down']\n",
      "58 ['down', 'right']\n",
      "59 ['down']\n",
      "60 ['up']\n",
      "61 ['left']\n",
      "62 ['left']\n",
      "63 ['left']\n",
      "64 ['left']\n",
      "65 ['left']\n",
      "66 ['down', 'right']\n",
      "67 ['down', 'right']\n",
      "68 ['down']\n",
      "69 ['down']\n",
      "70 ['up']\n",
      "71 ['left']\n",
      "72 ['left']\n",
      "73 ['left']\n",
      "74 ['left']\n",
      "75 ['right']\n",
      "76 ['down', 'right']\n",
      "77 ['down', 'right']\n",
      "78 ['down']\n",
      "79 ['down']\n",
      "80 ['up']\n",
      "81 ['left']\n",
      "82 ['left']\n",
      "83 ['left']\n",
      "84 ['left']\n",
      "85 ['down', 'right']\n",
      "86 ['right']\n",
      "87 ['right']\n",
      "88 ['down', 'right']\n",
      "89 ['down']\n",
      "90 ['up']\n",
      "91 ['left']\n",
      "92 ['left']\n",
      "93 ['left']\n",
      "94 ['left']\n",
      "95 ['right']\n",
      "96 ['right']\n",
      "97 ['right']\n",
      "98 ['right']\n"
     ]
    }
   ],
   "source": [
    "get_policy_iteration(10,10,0.9,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "       - Ser ut som att det börjar högt numerärt, sjunker sedan och försvinner efter på både 4x4, 8x8 och 10x10.\n",
    "    Vad händer med tiden?\n",
    "       - Den stiger från 0, sedan blev det 4 och slutligen 6 s.\n",
    "    Hur olika är policyerna?\n",
    "       - Ja, den visar tillstånden för policyerna, men det är fortfarande för många steg tills den når sitt mål."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f9d82a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3421506",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ac0a82f",
   "metadata": {},
   "source": [
    "## Del 3 - Värde iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd37880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterateValues(grid, V, policy, GAMMA, THETA):\n",
    "    converged = False\n",
    "    iterations = 0\n",
    "    while not converged:\n",
    "        DELTA = 0\n",
    "        for state in grid.stateSpace:\n",
    "            oldV = V[state]\n",
    "            newV = []            \n",
    "            for action in grid.actionSpace:\n",
    "                grid.setState(state)\n",
    "                newState, reward, _, _ = grid.step(action)\n",
    "                key = (newState, reward, state, action) \n",
    "                newV.append(grid.p[key]*(reward+GAMMA*V[newState]))                \n",
    "            newV = np.array(newV)\n",
    "            bestV = np.where(newV == newV.max())[0]\n",
    "            bestState = np.random.choice(bestV)\n",
    "            V[state] = newV[bestState]\n",
    "            DELTA = max(DELTA, np.abs(oldV-V[state]))\n",
    "            converged = True if DELTA < THETA else False\n",
    "        iterations = iterations + 1\n",
    "    print('iterations: ', iterations)\n",
    "\n",
    "    for state in grid.stateSpace:\n",
    "        newValues = []\n",
    "        actions = []\n",
    "        for action in grid.actionSpace:\n",
    "            grid.setState(state)\n",
    "            newState, reward, _, _ = grid.step(action)\n",
    "            key = (newState, reward, state, action)\n",
    "            newValues.append(grid.p[key]*(reward+GAMMA*V[newState]))\n",
    "            actions.append(action)\n",
    "        newValues = np.array(newValues)\n",
    "        bestActionIDX = np.where(newValues == newValues.max())[0]\n",
    "        bestActions = actions[np.random.choice(bestActionIDX)]\n",
    "        bestActions = actions[bestActionIDX[0]]\n",
    "        policy[state] = bestActions\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cd10bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_iteration(m, n, x, y):\n",
    "\n",
    "    grid = GridWorld(m,n)\n",
    "    THETA = x\n",
    "    GAMMA = y\n",
    "\n",
    "    # initialize V(s)\n",
    "    V = {}\n",
    "    for state in grid.stateSpacePlus:        \n",
    "        V[state] = 0\n",
    "\n",
    "    policy = {}\n",
    "    for state in grid.stateSpace:\n",
    "        policy[state] = [key for key in grid.actionSpace.keys()]\n",
    "\n",
    "    for i in range(2):\n",
    "        V, policy = iterateValues(grid, V, policy, GAMMA, THETA)\n",
    "        printV(V, grid)   \n",
    "        time.sleep(2)\n",
    "        clear_output(wait=True)\n",
    "    # Final   \n",
    "\n",
    "    printV(V, grid) \n",
    "    print()\n",
    "    for state in policy:\n",
    "        print(state, policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d888545",
   "metadata": {},
   "source": [
    "### Hands-on del 3.\n",
    "\n",
    "- Implementera ännu en förbättrad policyfunktion (iterateValues).\n",
    "- Testa koden för 4x4, 8x8 och 10x10 rutnät.\n",
    "    - Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "    - Vad händer med tiden?\n",
    "- Hur olika är policyerna för gamma=0.5,0.9 i ett 8x8-rutnät?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a6faea5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-1.90\t\n",
      "\n",
      "-1.90\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "\n",
      "1 left\n",
      "2 left\n",
      "3 down\n",
      "4 up\n",
      "5 up\n",
      "6 up\n",
      "7 down\n",
      "8 up\n",
      "9 up\n",
      "10 down\n",
      "11 down\n",
      "12 up\n",
      "13 right\n",
      "14 right\n"
     ]
    }
   ],
   "source": [
    "get_value_iteration(4,4,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "baa28900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "\n",
      "1 left\n",
      "2 left\n",
      "3 left\n",
      "4 up\n",
      "5 up\n",
      "6 up\n",
      "7 up\n",
      "8 up\n",
      "9 up\n",
      "10 up\n",
      "11 up\n",
      "12 up\n",
      "13 up\n",
      "14 up\n",
      "15 up\n",
      "16 up\n",
      "17 up\n",
      "18 up\n",
      "19 up\n",
      "20 up\n",
      "21 up\n",
      "22 up\n",
      "23 up\n",
      "24 up\n",
      "25 up\n",
      "26 up\n",
      "27 up\n",
      "28 up\n",
      "29 up\n",
      "30 up\n",
      "31 up\n",
      "32 up\n",
      "33 up\n",
      "34 up\n",
      "35 up\n",
      "36 up\n",
      "37 up\n",
      "38 up\n",
      "39 down\n",
      "40 up\n",
      "41 up\n",
      "42 up\n",
      "43 up\n",
      "44 up\n",
      "45 up\n",
      "46 down\n",
      "47 down\n",
      "48 up\n",
      "49 up\n",
      "50 up\n",
      "51 up\n",
      "52 up\n",
      "53 down\n",
      "54 down\n",
      "55 down\n",
      "56 up\n",
      "57 up\n",
      "58 up\n",
      "59 up\n",
      "60 right\n",
      "61 right\n",
      "62 right\n"
     ]
    }
   ],
   "source": [
    "get_value_iteration(8,8,0.9,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "84845efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "\n",
      "1 left\n",
      "2 left\n",
      "3 left\n",
      "4 up\n",
      "5 up\n",
      "6 up\n",
      "7 up\n",
      "8 up\n",
      "9 up\n",
      "10 up\n",
      "11 up\n",
      "12 up\n",
      "13 up\n",
      "14 up\n",
      "15 up\n",
      "16 up\n",
      "17 up\n",
      "18 up\n",
      "19 up\n",
      "20 up\n",
      "21 up\n",
      "22 up\n",
      "23 up\n",
      "24 up\n",
      "25 up\n",
      "26 up\n",
      "27 up\n",
      "28 up\n",
      "29 up\n",
      "30 up\n",
      "31 up\n",
      "32 up\n",
      "33 up\n",
      "34 up\n",
      "35 up\n",
      "36 up\n",
      "37 up\n",
      "38 up\n",
      "39 up\n",
      "40 up\n",
      "41 up\n",
      "42 up\n",
      "43 up\n",
      "44 up\n",
      "45 up\n",
      "46 up\n",
      "47 up\n",
      "48 up\n",
      "49 up\n",
      "50 up\n",
      "51 up\n",
      "52 up\n",
      "53 up\n",
      "54 up\n",
      "55 up\n",
      "56 up\n",
      "57 up\n",
      "58 up\n",
      "59 up\n",
      "60 up\n",
      "61 up\n",
      "62 up\n",
      "63 up\n",
      "64 up\n",
      "65 up\n",
      "66 up\n",
      "67 up\n",
      "68 up\n",
      "69 down\n",
      "70 up\n",
      "71 up\n",
      "72 up\n",
      "73 up\n",
      "74 up\n",
      "75 up\n",
      "76 up\n",
      "77 up\n",
      "78 down\n",
      "79 down\n",
      "80 up\n",
      "81 up\n",
      "82 up\n",
      "83 up\n",
      "84 up\n",
      "85 up\n",
      "86 up\n",
      "87 down\n",
      "88 down\n",
      "89 down\n",
      "90 up\n",
      "91 up\n",
      "92 up\n",
      "93 up\n",
      "94 up\n",
      "95 up\n",
      "96 right\n",
      "97 right\n",
      "98 right\n"
     ]
    }
   ],
   "source": [
    "get_value_iteration(10,10,0.9,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Vad händer med antalet iterationer som krävs för att konvergera?\n",
    "        Det kräver färre iterationer jämfört med del 2 och del 1, och ger bättre resultat med lägre negativa belöningar per steg.\n",
    "\n",
    "    Vad händer med tiden?\n",
    "        Utförandet är snabbare jämfört med tidigare skript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Hur olika är policyerna för gamma=0.5,0.9 i ett 8x8-rutnät?\n",
    "   - Med gamma = 0.9 fick snabbare och bättre policyerna mindre - poäng änn 0.5 som fick större värd.\n",
    "   - För gamma=0.5 verkar algoritmen föredra att röra sig mot det övre vänstra hörnet och att undvika nedre högra hörnet.\n",
    "   - För gamma=0.9 tenderar algoritmen att föredra att stanna på plats, med några undantag där 'right' föreslås.\n",
    "   - Resultaten indikerar att olika gamma-värden påverkar optimala strategier och rörelsemönster. En högre gamma (0.9) verkar leda till en mer  konservativ strategi där agensen är mindre benägen att riskera att förflytta sig till nya platser med högre potentiell belöning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-5.70\t\n",
      "\n",
      "-1.90\t-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-5.70\t-5.22\t\n",
      "\n",
      "-2.71\t-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-5.70\t-5.22\t-4.69\t\n",
      "\n",
      "-3.44\t-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-5.70\t-5.22\t-4.69\t-4.10\t\n",
      "\n",
      "-4.10\t-4.69\t-5.22\t-5.70\t-6.13\t-5.70\t-5.22\t-4.69\t-4.10\t-3.44\t\n",
      "\n",
      "-4.69\t-5.22\t-5.70\t-6.13\t-5.70\t-5.22\t-4.69\t-4.10\t-3.44\t-2.71\t\n",
      "\n",
      "-5.22\t-5.70\t-6.13\t-5.70\t-5.22\t-4.69\t-4.10\t-3.44\t-2.71\t-1.90\t\n",
      "\n",
      "-5.70\t-6.13\t-5.70\t-5.22\t-4.69\t-4.10\t-3.44\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-6.13\t-5.70\t-5.22\t-4.69\t-4.10\t-3.44\t-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "\n",
      "1 left\n",
      "2 left\n",
      "3 left\n",
      "4 left\n",
      "5 left\n",
      "6 left\n",
      "7 left\n",
      "8 left\n",
      "9 down\n",
      "10 up\n",
      "11 up\n",
      "12 up\n",
      "13 up\n",
      "14 up\n",
      "15 up\n",
      "16 up\n",
      "17 up\n",
      "18 up\n",
      "19 down\n",
      "20 up\n",
      "21 up\n",
      "22 up\n",
      "23 up\n",
      "24 up\n",
      "25 up\n",
      "26 up\n",
      "27 up\n",
      "28 down\n",
      "29 down\n",
      "30 up\n",
      "31 up\n",
      "32 up\n",
      "33 up\n",
      "34 up\n",
      "35 up\n",
      "36 up\n",
      "37 down\n",
      "38 down\n",
      "39 down\n",
      "40 up\n",
      "41 up\n",
      "42 up\n",
      "43 up\n",
      "44 up\n",
      "45 up\n",
      "46 down\n",
      "47 down\n",
      "48 down\n",
      "49 down\n",
      "50 up\n",
      "51 up\n",
      "52 up\n",
      "53 up\n",
      "54 up\n",
      "55 down\n",
      "56 down\n",
      "57 down\n",
      "58 down\n",
      "59 down\n",
      "60 up\n",
      "61 up\n",
      "62 up\n",
      "63 up\n",
      "64 down\n",
      "65 down\n",
      "66 down\n",
      "67 down\n",
      "68 down\n",
      "69 down\n",
      "70 up\n",
      "71 up\n",
      "72 up\n",
      "73 down\n",
      "74 down\n",
      "75 down\n",
      "76 down\n",
      "77 down\n",
      "78 down\n",
      "79 down\n",
      "80 up\n",
      "81 up\n",
      "82 down\n",
      "83 down\n",
      "84 down\n",
      "85 down\n",
      "86 down\n",
      "87 down\n",
      "88 down\n",
      "89 down\n",
      "90 up\n",
      "91 right\n",
      "92 right\n",
      "93 right\n",
      "94 right\n",
      "95 right\n",
      "96 right\n",
      "97 right\n",
      "98 right\n"
     ]
    }
   ],
   "source": [
    "get_value_iteration(10,10,0.5,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00\t-1.00\t-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-1.00\t-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-1.90\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t-1.00\t\n",
      "\n",
      "-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-2.71\t-1.90\t-1.00\t0.00\t\n",
      "\n",
      "--------------------\n",
      "\n",
      "1 left\n",
      "2 left\n",
      "3 left\n",
      "4 up\n",
      "5 up\n",
      "6 up\n",
      "7 up\n",
      "8 up\n",
      "9 up\n",
      "10 up\n",
      "11 up\n",
      "12 up\n",
      "13 up\n",
      "14 up\n",
      "15 up\n",
      "16 up\n",
      "17 up\n",
      "18 up\n",
      "19 up\n",
      "20 up\n",
      "21 up\n",
      "22 up\n",
      "23 up\n",
      "24 up\n",
      "25 up\n",
      "26 up\n",
      "27 up\n",
      "28 up\n",
      "29 up\n",
      "30 up\n",
      "31 up\n",
      "32 up\n",
      "33 up\n",
      "34 up\n",
      "35 up\n",
      "36 up\n",
      "37 up\n",
      "38 up\n",
      "39 up\n",
      "40 up\n",
      "41 up\n",
      "42 up\n",
      "43 up\n",
      "44 up\n",
      "45 up\n",
      "46 up\n",
      "47 up\n",
      "48 up\n",
      "49 up\n",
      "50 up\n",
      "51 up\n",
      "52 up\n",
      "53 up\n",
      "54 up\n",
      "55 up\n",
      "56 up\n",
      "57 up\n",
      "58 up\n",
      "59 up\n",
      "60 up\n",
      "61 up\n",
      "62 up\n",
      "63 up\n",
      "64 up\n",
      "65 up\n",
      "66 up\n",
      "67 up\n",
      "68 up\n",
      "69 down\n",
      "70 up\n",
      "71 up\n",
      "72 up\n",
      "73 up\n",
      "74 up\n",
      "75 up\n",
      "76 up\n",
      "77 up\n",
      "78 down\n",
      "79 down\n",
      "80 up\n",
      "81 up\n",
      "82 up\n",
      "83 up\n",
      "84 up\n",
      "85 up\n",
      "86 up\n",
      "87 down\n",
      "88 down\n",
      "89 down\n",
      "90 up\n",
      "91 up\n",
      "92 up\n",
      "93 up\n",
      "94 up\n",
      "95 up\n",
      "96 right\n",
      "97 right\n",
      "98 right\n"
     ]
    }
   ],
   "source": [
    "get_value_iteration(10,10,0.9,0.9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
